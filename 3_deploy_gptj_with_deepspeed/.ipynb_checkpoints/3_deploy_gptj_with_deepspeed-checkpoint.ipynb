{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e05319",
   "metadata": {},
   "source": [
    "# Large model inference with Deepspeed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e1d383",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to run inference for large models with DeepSpeed locally and then deploy it in a SageMaker Inference Endpoint. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c607a216",
   "metadata": {},
   "source": [
    "<font color=\"red\"> Note that you need to run the notebook `1_train_gptj_smp_tensor_parallel` first to produce the model artifact that will be used in this notebook. Refer to the variable/cell `model_location`. This needs to be set to the model_s3_uri in the download stage below.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606cd93c",
   "metadata": {},
   "source": [
    "## 1. Download trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39940ce0",
   "metadata": {},
   "source": [
    "First, let's clear some space on the notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1e78a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reclaimed space: 0B\r\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p27\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/chainer_p\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/chainer_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/chainer_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/mxnet_latest_p37/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/mxnet_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/mxnet_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/python2/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/python3/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/pytorch_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/pytorch_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/tensorflow2_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/tensorflow_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/tensorflow_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/R/\n",
    "!docker system prune -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc744b",
   "metadata": {},
   "source": [
    "Download the trained model for local testing. Set the model_s3_uri for the trained model. It should be of the form\n",
    "`s3://sagemaker-us-west-2-855988369404/smp-tensorparallel-outputdir/smp-gpt-j-xl-p38xl-tp4-pp1-bs8-2022-06-22-21-03-26-813/output/model.tar.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02af4a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s3_uri = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4bd43d",
   "metadata": {},
   "source": [
    "Next cell controls which local path to use for fetching the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "720eecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_dir = \"./model/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a2c6e",
   "metadata": {},
   "source": [
    "Next, we download the model.tar.gz file produced by SageMaker training with the previous GPT-J notebook, then we extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "640db704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ MODEL_PATH=s3://sagemaker-us-west-2-855988369404/smp-tensorparallel-outputdir/smp-gpt-j-xl-p38xl-tp4-pp1-bs8-2022-06-22-21-03-26-813/6b_output/model.tar.gz\n",
      "+ DIR=/dev/shm/model/\n",
      "+ rm -rf /dev/shm/model/\n",
      "+ mkdir -p /dev/shm/model/\n",
      "+ aws s3 cp s3://sagemaker-us-west-2-855988369404/smp-tensorparallel-outputdir/smp-gpt-j-xl-p38xl-tp4-pp1-bs8-2022-06-22-21-03-26-813/6b_output/model.tar.gz /dev/shm/model/\n",
      "download: s3://sagemaker-us-west-2-855988369404/smp-tensorparallel-outputdir/smp-gpt-j-xl-p38xl-tp4-pp1-bs8-2022-06-22-21-03-26-813/6b_output/model.tar.gz to ../../../../dev/shm/model/model.tar.gz\n",
      "+ cd /dev/shm/model/\n",
      "+ tar -xvf model.tar.gz\n",
      "gptj.pt\n",
      "special_tokens_map.json\n",
      "config.json\n",
      "code/\n",
      "code/inference.py\n",
      "added_tokens.json\n",
      "merges.txt\n",
      "tokenizer_config.json\n",
      "tokenizer.json\n",
      "vocab.json\n",
      "+ rm model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "! chmod +x ./download.sh\n",
    "! ./download.sh $local_model_dir $model_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b218d55e",
   "metadata": {},
   "source": [
    "# 2. Prepare docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29915a2",
   "metadata": {},
   "source": [
    "We have a `build.sh` bash script which performs the following steps:\n",
    "\n",
    "* Makes `serve` executable and builds our docker image\n",
    "* Optionally, runs the container for local testing\n",
    "\n",
    "Run with local testing using the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c711dded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  23.04kB\n",
      "Step 1/13 : FROM pytorch/pytorch:1.8.1-cuda11.1-cudnn8-devel\n",
      " ---> 7afd9b52a068\n",
      "Step 2/13 : LABEL com.amazon.image.authors.email=\"sage-learner@amazon.com\"\n",
      " ---> Using cache\n",
      " ---> ebe03e021f48\n",
      "Step 3/13 : LABEL com.amazon.image.authors.name=\"Amazon AI\"\n",
      " ---> Using cache\n",
      " ---> 2dc0d8bb8ea4\n",
      "Step 4/13 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 6e2de100516d\n",
      "Step 5/13 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> 8c7498c062e1\n",
      "Step 6/13 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> ceb5844ea6d5\n",
      "Step 7/13 : ARG DEBIAN_FRONTEND=noninteractive\n",
      " ---> Using cache\n",
      " ---> 396b707be231\n",
      "Step 8/13 : ENV TZ=Etc/UTC\n",
      " ---> Using cache\n",
      " ---> f9d427bb8835\n",
      "Step 9/13 : RUN apt-key del 7fa2af80     && rm /etc/apt/sources.list.d/nvidia-ml.list /etc/apt/sources.list.d/cuda.list     && apt-get -y update && apt-get install -y --no-install-recommends         wget     && wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-keyring_1.0-1_all.deb     && dpkg -i cuda-keyring_1.0-1_all.deb     && apt-get -y update && apt-get install -y --no-install-recommends         python3-pip         python3-setuptools         nginx         ca-certificates     && apt-get -y autoremove     && apt-get clean autoclean     && rm -fr /var/lib/apt/lists/{apt,dpkg,cache,log} /tmp/* /var/tmp/*\n",
      " ---> Using cache\n",
      " ---> a281e879f635\n",
      "Step 10/13 : RUN ln -s /usr/bin/python3 /usr/bin/python\n",
      " ---> Using cache\n",
      " ---> 681175d41b38\n",
      "Step 11/13 : RUN pip --version     && pip --no-cache-dir install         transformers==4.15         deepspeed==0.5.10         flask==2.1.2         gunicorn==20.1.0\n",
      " ---> Using cache\n",
      " ---> bca77dec93cd\n",
      "Step 12/13 : COPY ./src /opt/program\n",
      " ---> 8b4456bb1bf3\n",
      "Step 13/13 : WORKDIR /opt/program\n",
      " ---> Running in 7ca8a977f400\n",
      "Removing intermediate container 7ca8a977f400\n",
      " ---> 2a93e8f642aa\n",
      "Successfully built 2a93e8f642aa\n",
      "Successfully tagged gptj-inference-endpoint:latest\n",
      "0d7270e9dc3afd772fccebb456164d5c488226c7311b47074eb41b44ffc81bbf\n",
      "REPOSITORY                                                                  TAG                                                    IMAGE ID       CREATED                  SIZE\n",
      "gptj-inference-endpoint                                                     latest                                                 2a93e8f642aa   Less than a second ago   16.7GB\n",
      "855988369404.dkr.ecr.us-west-2.amazonaws.com/gptj-inference-endpoint        latest                                                 bd38b77c052f   5 hours ago              16.7GB\n",
      "763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training   1.10-transformers4.17-gpu-py38-cu113-ubuntu20.04       7fa283c171a6   4 weeks ago              14.2GB\n",
      "763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training   1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04   7fa283c171a6   4 weeks ago              14.2GB\n",
      "pytorch/pytorch                                                             1.8.1-cuda11.1-cudnn8-devel                            7afd9b52a068   15 months ago            16.5GB\n",
      "Existing model directory detected...\n",
      "0d7270e9dc3a\n",
      "Starting container for local deployemnt\n",
      "2d9dca7f4bc2d27cf99407dca5218b6adc8357c6faf23eabf8629a8f251c4310\n"
     ]
    }
   ],
   "source": [
    "! ./build.sh gptj-inference-endpoint $local_model_dir test_local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010fc64d",
   "metadata": {},
   "source": [
    "Or, to run without local testing, run:\n",
    "\n",
    "```sh\n",
    "./build.sh gptj-inference-endpoint\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d324ffc7",
   "metadata": {},
   "source": [
    "To test the endpoint, you can run the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be266778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import sys \n",
    "\n",
    "URL = 'http://127.0.0.1:8080/invocations'\n",
    "HEADERS = {'Content-type': 'application/json', 'Accept': '*/*'}\n",
    "\n",
    "def test_endpoint(text, parameters):\n",
    "    \n",
    "    data = {\n",
    "        \"inputs\":{\n",
    "            \"text_inputs\": text,\n",
    "            \"parameters\": parameters\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    payload = json.dumps(data)\n",
    "    response = requests.post(URL, json=data, headers=HEADERS)\n",
    "    \n",
    "    return(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "996d4e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
      "\n",
      "Prompt: A scary story about a haunted mouse\n",
      "Story: On a dark and stormy night, the mouse crept in the shadows. _____________________ In a room that was so cold, it hurt to breathe. Everything seemed dead and deserted - like an old building abandoned after the owner took off for some faraway land. The little creature shivered like a leaf blown by the wind. __________ But no one had lived there for years - not even the mice.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The squeaking of the floorboards grew louder as he made his way toward the stairs. He was not afraid of walking through the rooms because they had become his new home. His mother never saw him again. _________ Now it was time to find out if something new would live here with him. He put his hand on one of the steps, preparing himself to start climbing when suddenly from above there came a shrill cry. The mouse froze, expecting any moment to hear the crack of a rifle shot or the shouts of men rushing upstairs after him. Even worse were the screams of a terrified woman coming from somewhere close at hand. With trembling\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_new_tokens\":200,\n",
    "    \"min_tokens\": 100,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"top_p\": 500,\n",
    "    }\n",
    "\n",
    "response = json.loads(test_endpoint(text, parameters))\n",
    "print(response['response'][0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795b011b",
   "metadata": {},
   "source": [
    "# 3. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30053aa",
   "metadata": {},
   "source": [
    "When you're satisfied with your container, you can rebuild and push your container to AWS ECR using the `push_to_ecr.sh` script.\n",
    "\n",
    "For example, to push the image we built above, named \"gptj-inference-endpoint\", you can use the `push_to_ecr.sh` script, which requires the name of your docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0877a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "The push refers to repository [855988369404.dkr.ecr.us-west-2.amazonaws.com/gptj-inference-endpoint]\n",
      "\n",
      "\u001b[1B17d3cc2a: Preparing \n",
      "\u001b[1B26a856d9: Preparing \n",
      "\u001b[1Ba9bb730d: Preparing \n",
      "\u001b[1B81b1853a: Preparing \n",
      "\u001b[1Bbb7de61b: Preparing \n",
      "\u001b[1B263f678e: Preparing \n",
      "\u001b[1Bd43a62a1: Preparing \n",
      "\u001b[1Bfc4a44ce: Preparing \n",
      "\u001b[1Bbc5acecf: Preparing \n",
      "\u001b[1B2fb01f89: Preparing \n",
      "\u001b[1B6813b3ac: Preparing \n",
      "\u001b[1B30aca740: Preparing \n",
      "\u001b[1Bcd672bd2: Preparing \n",
      "\u001b[1B8881187d: Preparing \n",
      "\u001b[1B5df75b44: Preparing \n",
      "\u001b[16B7d3cc2a: Pushed lready exists 4kB6A\u001b[2K\u001b[7A\u001b[2K\u001b[3A\u001b[2K\u001b[16A\u001b[2Klatest: digest: sha256:9900a842095286bbf30a3ce9c6e9fc156ba9578ecf03f923b132757b2e552bf8 size: 3687\n"
     ]
    }
   ],
   "source": [
    "! chmod +x push_to_ecr.sh\n",
    "! ./push_to_ecr.sh gptj-inference-endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106e95ed",
   "metadata": {},
   "source": [
    "First, this script will push your image to ECR. For reference later, note the address of the repository that the container is pushed to. It should appear below the line `Login Succeeded` in the output from the call to `push_to_ecr.sh`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73337512",
   "metadata": {},
   "source": [
    "# 4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a0827",
   "metadata": {},
   "source": [
    "Now, you can deploy your endpoint as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c71123",
   "metadata": {},
   "source": [
    "### 4.1 Initialize configuration variables\n",
    "\n",
    "If you run into the error that endpoint already exists on a rerun, please change the model_name and endpoint_name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74782eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import RealTimePredictor\n",
    "import time \n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Specify s3uri for model.tar.gz\n",
    "model_data = model_s3_uri\n",
    "\n",
    "# Specify path to gptj-inference-endpoint image in ECR\n",
    "image = \"855988369404.dkr.ecr.us-west-2.amazonaws.com/gptj-inference-endpoint\"\n",
    "\n",
    "# Specify sagemaker model_name\n",
    "sm_model_name = \"gptj-completion-gpu-test7\"\n",
    "\n",
    "# Specify endpoint_name\n",
    "endpoint_name = \"gptj-completion-gpu-test7\"\n",
    "\n",
    "# Specify instance_type\n",
    "instance_type = 'ml.g4dn.2xlarge'\n",
    "\n",
    "# Specify initial_instance_count\n",
    "initial_instance_count = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423f307f",
   "metadata": {},
   "source": [
    "### 4.2 Initialize endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56812ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----"
     ]
    }
   ],
   "source": [
    "sm_model = Model(model_data = model_data, \n",
    "                        image_uri = image,\n",
    "                        role = role,\n",
    "                        predictor_cls=RealTimePredictor,\n",
    "                        name = sm_model_name)\n",
    "\n",
    "predictor = sm_model.deploy(\n",
    "        instance_type=instance_type,\n",
    "        initial_instance_count=1,\n",
    "        endpoint_name = endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ad8d80",
   "metadata": {},
   "source": [
    "### 4.3 Query model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95330a0d",
   "metadata": {},
   "source": [
    "To query your endpoint, you can use the code below. Also, remember that you can pass any parameters accepted by the HuggingFace `\"text-generation\"` pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61365770",
   "metadata": {},
   "source": [
    "#### Initialize asynchronous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1de461c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json \n",
    "\n",
    "# Get the boto3 session and sagemaker client, as well as the current execution role\n",
    "sess = boto3.Session()\n",
    "\n",
    "# Specify your AWS Region\n",
    "aws_region=sess.region_name\n",
    "\n",
    "\n",
    "# Create a low-level client representing Amazon SageMaker Runtime\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\", region_name=aws_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af9b9738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.6 ms, sys: 542 µs, total: 15.1 ms\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\":200,\n",
    "    \"min_tokens\": 100,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"top_p\": 500,\n",
    "    }\n",
    "\n",
    "data = {\n",
    "    \"inputs\": {\n",
    "        \"text_inputs\": text,\n",
    "        \"parameters\": parameters\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "body = json.dumps(data)\n",
    "\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint( \n",
    "        EndpointName=endpoint_name, \n",
    "        Body = body, \n",
    "        ContentType = 'application/json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77851c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.9 ms, sys: 35 µs, total: 14.9 ms\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "body = json.dumps(data)\n",
    "\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint( \n",
    "        EndpointName=endpoint_name, \n",
    "        Body = body, \n",
    "        ContentType = 'application/json'\n",
    ")\n",
    "\n",
    "result = json.loads(response['Body'].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7108e9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': [{'generated_text': 'This is a creative writing exercise. Below, you\\'ll be given a prompt. Your story should be based on the prompt.\\n\\nPrompt: A scary story about a haunted mouse\\nStory: On a dark and stormy night, the mouse crept in the shadows.  \\n---\\n\\n 1. The mouse began to pace up and down his little cage, but it did not seem as if he was doing anything. After several minutes of pacing, he sat down for a moment.  \\n2. He looked around at the darkness with narrowed eyes. \"I can\\'t take this anymore.\" The mouse began to sniffle. \"You\\'re all alone out here,\" the mouse said, hiccupping. \"I feel so scared.\"  \\n3. Suddenly, the mouse heard footsteps coming closer and closer; he then saw a figure walking into the room. His heart raced with fear, and he ran to the corner of the room that contained his food dish.  \\n4. The figure was wearing a hooded sweatshirt and black jeans. He looked like a teenager. The mouse could just make out the outline of the person\\'s face through the fabric.  \\n5. \"The mouse started to tremble,\" the man whispered'}],\n",
       " 'status': 200}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7ac186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
