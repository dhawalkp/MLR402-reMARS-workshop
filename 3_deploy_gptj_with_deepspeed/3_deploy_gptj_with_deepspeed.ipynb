{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac02a68",
   "metadata": {},
   "source": [
    "# Large model inference with Deepspeed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ecf560",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to run inference for large models with DeepSpeed locally and then deploy it in a SageMaker Inference Endpoint. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb57a9be",
   "metadata": {},
   "source": [
    "<font color=\"red\"> Note that you need to run the notebook `1_train_gptj_smp_tensor_parallel` first to produce the model artifact that will be used in this notebook. Refer to the variable/cell `model_location`. This needs to be set to the model_s3_uri in the download stage below.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacab47f",
   "metadata": {},
   "source": [
    "## 1. Download trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4f4de1",
   "metadata": {},
   "source": [
    "First, let's clear some space on the notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e5c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p27\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/chainer_p\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/chainer_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/chainer_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/mxnet_latest_p37/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/mxnet_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/mxnet_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/python2/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/python3/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/pytorch_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/pytorch_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/tensorflow2_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/tensorflow_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/tensorflow_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/R/\n",
    "!docker system prune -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8185548",
   "metadata": {},
   "source": [
    "Download the trained model for local testing. Set the model_s3_uri for the trained model. It should be of the form\n",
    "`s3://sagemaker-us-west-2-855988369404/smp-tensorparallel-outputdir/smp-gpt-j-xl-p38xl-tp4-pp1-bs8-2022-06-22-21-03-26-813/output/model.tar.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccc1e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s3_uri = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c85943",
   "metadata": {},
   "source": [
    "Next cell controls which local path to use for fetching the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf5467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_dir = \"./model/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aeb9c9",
   "metadata": {},
   "source": [
    "Next, we download the model.tar.gz file produced by SageMaker training with the previous GPT-J notebook, then we extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f411be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x ./download.sh\n",
    "! ./download.sh $local_model_dir $model_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df83848e",
   "metadata": {},
   "source": [
    "# 2. Prepare docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9875e5ca",
   "metadata": {},
   "source": [
    "We have a `build.sh` bash script which performs the following steps:\n",
    "\n",
    "* Makes `serve` executable and builds our docker image\n",
    "* Optionally, runs the container for local testing\n",
    "\n",
    "Run with local testing using the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f08129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./build.sh gptj-inference-endpoint $local_model_dir test_local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69f6566",
   "metadata": {},
   "source": [
    "Or, to run without local testing, run:\n",
    "\n",
    "```sh\n",
    "./build.sh gptj-inference-endpoint\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c212b65",
   "metadata": {},
   "source": [
    "To test the endpoint, you can run the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e7ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import sys \n",
    "\n",
    "URL = 'http://127.0.0.1:8080/invocations'\n",
    "HEADERS = {'Content-type': 'application/json', 'Accept': '*/*'}\n",
    "\n",
    "def test_endpoint(text, parameters):\n",
    "    \n",
    "    data = {\n",
    "        \"inputs\":{\n",
    "            \"text_inputs\": text,\n",
    "            \"parameters\": parameters\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    payload = json.dumps(data)\n",
    "    response = requests.post(URL, json=data, headers=HEADERS)\n",
    "    \n",
    "    return(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de385ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_new_tokens\":200,\n",
    "    \"min_tokens\": 100,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"top_p\": 500,\n",
    "    }\n",
    "\n",
    "response = json.loads(test_endpoint(text, parameters))\n",
    "print(response['response'][0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee3d91",
   "metadata": {},
   "source": [
    "# 3. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ca297",
   "metadata": {},
   "source": [
    "When you're satisfied with your container, you can rebuild and push your container to AWS ECR using the `push_to_ecr.sh` script.\n",
    "\n",
    "For example, to push the image we built above, named \"gptj-inference-endpoint\", you can use the `push_to_ecr.sh` script, which requires the name of your docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d57495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "new_s3_uri = os.path.join(os.path.dirname(model_s3_uri), \"infer_model.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbce6439",
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x push_to_ecr.sh\n",
    "! ./push_to_ecr.sh gptj-inference-endpoint $local_model_dir $new_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd48a16",
   "metadata": {},
   "source": [
    "First, this script will push your image to ECR. For reference later, note the address of the repository that the container is pushed to. It should appear below the line `Login Succeeded` in the output from the call to `push_to_ecr.sh`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99130d37",
   "metadata": {},
   "source": [
    "# 4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec0d958",
   "metadata": {},
   "source": [
    "Now, you can deploy your endpoint as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7481806",
   "metadata": {},
   "source": [
    "### 4.1 Initialize configuration variables\n",
    "\n",
    "If you run into the error that endpoint already exists on a rerun, please change the model_name and endpoint_name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d68c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import RealTimePredictor\n",
    "import time \n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Specify s3uri for model.tar.gz\n",
    "model_data = new_s3_uri\n",
    "\n",
    "# Specify path to gptj-inference-endpoint image in ECR\n",
    "image = \"\"\n",
    "\n",
    "# Specify sagemaker model_name\n",
    "sm_model_name = \"gptj-completion-gpu-test\"\n",
    "\n",
    "# Specify endpoint_name\n",
    "endpoint_name = \"gptj-completion-gpu-test\"\n",
    "\n",
    "# Specify instance_type\n",
    "instance_type = 'ml.g4dn.2xlarge'\n",
    "\n",
    "# Specify initial_instance_count\n",
    "initial_instance_count = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed4c4d",
   "metadata": {},
   "source": [
    "### 4.2 Initialize endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70984ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class RealTimePredictor has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "sm_model = Model(model_data = model_data, \n",
    "                        image_uri = image,\n",
    "                        role = role,\n",
    "                        predictor_cls=RealTimePredictor,\n",
    "                        name = sm_model_name)\n",
    "\n",
    "predictor = sm_model.deploy(\n",
    "        instance_type=instance_type,\n",
    "        initial_instance_count=1,\n",
    "        endpoint_name = endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d66c4",
   "metadata": {},
   "source": [
    "### 4.3 Query model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d0885",
   "metadata": {},
   "source": [
    "To query your endpoint, you can use the code below. Also, remember that you can pass any parameters accepted by the HuggingFace `\"text-generation\"` pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fec389",
   "metadata": {},
   "source": [
    "#### Initialize asynchronous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81bf8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json \n",
    "\n",
    "# Get the boto3 session and sagemaker client, as well as the current execution role\n",
    "sess = boto3.Session()\n",
    "\n",
    "# Specify your AWS Region\n",
    "aws_region=sess.region_name\n",
    "\n",
    "\n",
    "# Create a low-level client representing Amazon SageMaker Runtime\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\", region_name=aws_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e81e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\":200,\n",
    "    \"min_tokens\": 100,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"top_p\": 500,\n",
    "    }\n",
    "\n",
    "data = {\n",
    "    \"inputs\": {\n",
    "        \"text_inputs\": text,\n",
    "        \"parameters\": parameters\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "body = json.dumps(data)\n",
    "\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint( \n",
    "        EndpointName=endpoint_name, \n",
    "        Body = body, \n",
    "        ContentType = 'application/json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab76fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "body = json.dumps(data)\n",
    "\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint( \n",
    "        EndpointName=endpoint_name, \n",
    "        Body = body, \n",
    "        ContentType = 'application/json'\n",
    ")\n",
    "\n",
    "result = json.loads(response['Body'].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c6dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065baac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
