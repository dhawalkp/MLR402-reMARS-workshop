{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Deploy GPT-J-6B model using Tensor Parallelism approach within SageMaker Model Parallel Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks you through how to use the tensor parallelism feature provided by the SageMaker model parallelism library. You'll learn how to train the GPT-J model with tensor parallelism on the GLUE sst2 dataset.\n",
    "\n",
    "This notebook walks you through how to train the [EleutherAI's](https://www.eleuther.ai/) [GPT-J](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/) model with SageMaker's model parallelism.\n",
    "EleutherAI released GPT-J 6B, an open-source alternative to [OpenAIs GPT-3](https://openai.com/blog/gpt-3-apps/). [GPT-J 6B](https://huggingface.co/EleutherAI/gpt-j-6B) is the 6 billion parameter successor to EleutherAIs GPT-NEO family, a family of transformer-based language models based on the GPT architecture for text generation.\n",
    "\n",
    "EleutherAI's primary goal is to train a model that is equivalent in size to GPT⁠-⁠3 and make it available to the public under an open license.\n",
    "Over the last few months, GPT-J gained a lot of interest from Researchers, Data Scientists, and even Software Developers, but it remained very challenging to fine tune GPT-J.\n",
    "\n",
    "The weights of the 6 billion parameter model represent a ~24GB memory footprint. To load it in float32, one would need at least 2x model size CPU RAM: 1x for initial weights and another 1x to load the checkpoint. Apart from the model parameters, there are the gradients, optimizer states, and activations taking memory, so the actual memory usage might be significantly higher than 48GB. Just as an example, with Adam optimizer and FP32 training, the use from parameters, gradients and optimizer states might be 96GB+, and activation memory footprint would be even more than this, so the total memory usage might be easily larger than 200 GB.\n",
    "\n",
    "![GPT-J Memory requirements](img/GPT-J-Memory.png)\n",
    "\n",
    "In this notebook, you will learn how to easily fine tune GPT-J using Amazon SageMaker and Hugging Face on NVIDIA GPU instances. The notebook demonstrates the use of Tensor Parallel approach of SageMaker Model Parallel library.\n",
    "\n",
    "This notebook depends on the following files and folders:\n",
    "\n",
    "1. `train_gptj_smp_tensor_parallel_script.py`: This is an entrypoint script that is passed to the PyTorch estimator in the notebook instructions. This script is responsible for end to end training of the GPT-J model with SMP. The script has additional comments at places where the SMP API is used.\n",
    "2. `fp16`: This folder is used for 16-bit float training, which contains a fp16 optimizer and various fp16 utilities.\n",
    "3. `learning_rates.py`: This contains the functions for learning rate schedule.\n",
    "4. `requirements.txt`: This will install the dependencies, like the right version of huggingface transformers.\n",
    "5. `memory_tracker.py`: This contains a function to print the memory status.\n",
    "\n",
    "\n",
    "## SageMaker Distributed Training \n",
    "\n",
    "SageMaker provides distributed training libraries for data parallelism and model parallelism. The libraries are optimized for the SageMaker training environment, help adapt your distributed training jobs to SageMaker, and improve training speed and throughput.\n",
    "\n",
    "### Approaches\n",
    "\n",
    "![SageMaker Distributed Training Approaches](img/TypesOfDistributedTraining.png)\n",
    "\n",
    "\n",
    "### SageMaker Model Parallel\n",
    "\n",
    "Model parallelism is the process of splitting a model up between multiple devices or nodes (such as GPU-equipped instances) and creating an efficient pipeline to train the model across these devices to maximize GPU utilization.\n",
    "\n",
    "Increasing deep learning model size (layers and parameters) can result in better accuracy. However, there is a limit to the maximum model size you can fit in a single GPU. When training deep learning models, GPU memory limitations can be a bottleneck in the following ways:\n",
    "\n",
    "1. They can limit the size of the model you train. Given that larger models tend to achieve higher accuracy, this directly translates to trained model accuracy.\n",
    "\n",
    "2. They can limit the batch size you train with, leading to lower GPU utilization and slower training.\n",
    "\n",
    "To overcome the limitations associated with training a model on a single GPU, you can use model parallelism to distribute and train your model on multiple computing devices.\n",
    "\n",
    "### Core features of SageMaker Model Parallel \n",
    "\n",
    "1. [Automated Model Splitting](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html): When you use SageMaker's model parallel library, you can take advantage of automated model splitting, also referred to as automated model partitioning. The library uses a partitioning algorithm that balances memory, minimizes communication between devices, and optimizes performance. You can configure the automated partitioning algorithm to optimize for speed or memory.\n",
    "\n",
    "2. [Pipeline Execution Schedule](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html): A core feature of SageMaker's distributed model parallel library is pipelined execution, which determines the order in which computations are made and data is processed across devices during model training. Pipelining is a technique to achieve true parallelization in model parallelism, by having the GPUs compute simultaneously on different data samples, and to overcome the performance loss due to sequential computation.\n",
    "\n",
    "Pipelining is based on splitting a mini-batch into microbatches, which are fed into the training pipeline one-by-one and follow an execution schedule defined by the library runtime. A microbatch is a smaller subset of a given training mini-batch. The pipeline schedule determines which microbatch is executed by which device for every time slot.\n",
    "\n",
    "In addition to its [core features](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html), the SageMaker distributed model parallel library offers [memory-saving features](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch.html) for training deep learning models with PyTorch: [tensor parallelism](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-tensor-parallelism.html), [optimizer state sharding](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-optimizer-state-sharding.html), [activation checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-checkpointing.html), and [activation offloading](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-offloading.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Model Parallel configuration\n",
    "\n",
    "Please refer to all the [configuration parameters](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html) related to SageMaker Distributed Training.\n",
    "\n",
    "As we are going to use PyTorch and Hugging Face for training GPT-J, it is important to understand all the SageMaker Distributed configuration parameters specific to PyTorch [here](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html#pytorch-specific-parameters).\n",
    "\n",
    "#### Important\n",
    "\n",
    "`process_per_host` must not be greater than the number of GPUs per instance and typically will be equal to the number of GPUs per instance.\n",
    "\n",
    "#### SageMaker Tensor Parallel\n",
    "\n",
    "Tensor parallelism splits individual layers, or nn.Modules, across devices, to be run in parallel. The following figure shows the simplest example of how the library splits a model with four layers to achieve two-way tensor parallelism (\"tensor_parallel_degree\": 2). The layers of each model replica are bisected and distributed into two GPUs. In this example case, the model parallel configuration also includes \"pipeline_parallel_degree\": 1 and \"ddp\": True (uses PyTorch DistributedDataParallel package in the background), so the degree of data parallelism becomes eight. The library manages communication across the tensor-distributed model replicas.\n",
    "\n",
    "![SageMaker Distributed Training Approaches](img/smdmp-tensor-parallel-only.png)\n",
    "\n",
    "The usefulness of this feature is in the fact that you can select specific layers or a subset of layers to apply tensor parallelism. To dive deep into tensor parallelism and other memory-saving features for PyTorch, and to learn how to set a combination of pipeline and tensor parallelism, see Extended Features of the SageMaker Model Parallel Library for PyTorch.\n",
    "\n",
    "\n",
    "\n",
    "#### Additional Resources\n",
    "If you are a new user of Amazon SageMaker, you may find the following helpful to learn more about SMP and using SageMaker with PyTorch.\n",
    "\n",
    "1. To learn more about the SageMaker model parallelism library, see [Model Parallel Distributed Training with SageMaker Distributed](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html).\n",
    "\n",
    "2. To learn more about using the SageMaker Python SDK with PyTorch, see Using [PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html).\n",
    "\n",
    "3. To learn more about launching a training job in Amazon SageMaker with your own training image, see [Use Your Own Training Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with this notebook\n",
    "*Note:* You have two options on how to run this notebook.  \n",
    "    1. You can use the _Amazon SageMaker local mode_ to train on your notebook instance  \n",
    "    2. You can the _managed Amazon SageMaker training_ environment to train your model in a separate cluster of EC2 instance(s)\n",
    "\n",
    "The below cells check the notebook instance you are using and set the `local_training` flag to `True` if you are on a multi-GPU SageMaker notebook instance and can therefore train the model locally. If you have a CPU based SageMaker notebook instance we set the `local_training` flag to `False` and you will use SageMaker managed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "def get_notebook_name():\n",
    "    import json\n",
    "    log_path = '/opt/ml/metadata/resource-metadata.json'\n",
    "    with open(log_path, 'r') as logs:\n",
    "        _logs = json.load(logs)\n",
    "    return _logs['ResourceName']\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "response = client.describe_notebook_instance(\n",
    "    NotebookInstanceName=get_notebook_name())\n",
    "    # set to the number of GPUs on that instance\n",
    "instance_type= response['InstanceType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running this notebook on a multi GPU SageMaker notebook instance. For the purpose of this workshop you will use SageMaker local training.\n",
      "Stored 'local_training' (bool)\n"
     ]
    }
   ],
   "source": [
    "if instance_type in ['ml.p3.16xlarge', 'ml.p3.8xlarge']:\n",
    "    local_training = False\n",
    "    print(\"You are running this notebook on a multi GPU SageMaker notebook instance. For the purpose of this workshop you will use SageMaker local training.\")\n",
    "else:\n",
    "    local_training = False\n",
    "    print(\"You are running this notebook on a CPU based SageMaker notebook instance. For the purpose of this workshop you will use SageMaker remote/managed training.\")\n",
    "%store local_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Upgrade Libraries\n",
    "\n",
    "The SageMaker model parallelism library's tensor parallelism feature requires the SageMaker Python SDK and the SageMaker Experiments library. Run the following cell to install or upgrade the libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** To finish applying the changes, you must restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p27\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/chainer_p\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/chainer_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/chainer_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/mxnet_latest_p37/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/mxnet_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/mxnet_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/python2/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/python3/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/pytorch_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/pytorch_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/tensorflow2_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/tensorflow_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/tensorflow_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/R/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # run once, restart kernel, then comment out this cell\n",
    "# ! pip install -qU pip\n",
    "# ! pip install -qU \"sagemaker>=2,<3\"\n",
    "# ! pip install -qU sagemaker-experiments\n",
    "# ! pip install -qU transformers datasets\n",
    "# ! pip install -qU 'sagemaker[local]' --upgrade\n",
    "\n",
    "# import IPython\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Important</b> After you run the above cell, comment it out for future runs.\n",
    "\n",
    "Import and check if the SageMaker Python SDK version is successfully set to the latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.96.0\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Initialization\n",
    "\n",
    "Throughout this example, you'll use a training script of GPT-J model and a text dataset.\n",
    "\n",
    "Based on the `local_training` flag, you need to setup SageMaker differently. The below cells check this flag and initailize SageMaker appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieve value of local_training\n",
    "%store -r local_training\n",
    "local_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_training:\n",
    "    # clear all images\n",
    "    !docker system prune -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if local_training:\n",
    "    from sagemaker.local.local_session import LocalSession\n",
    "    sagemaker_session = LocalSession()\n",
    "    tmp_dir = os.getcwd() + \"/tmp/\"\n",
    "    sagemaker_session.config = {'local': {'container_root': tmp_dir,'local_code': True}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to import SageMaker modules and retrieve information of your current SageMaker work environment: your AWS account ID, the AWS Region you are using to run the notebook, and the ARN of your Amazon SageMaker execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Execution Role:arn:aws:iam::855988369404:role/SageMakerRole\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS account:855988369404\n",
      "AWS region:us-west-2\n",
      "\n",
      "Default bucket for this session:  sagemaker-us-west-2-855988369404\n",
      "CPU times: user 537 ms, sys: 22.8 ms, total: 560 ms\n",
      "Wall time: 1.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "import boto3\n",
    "\n",
    "def get_notebook_name():\n",
    "    import json\n",
    "    log_path = '/opt/ml/metadata/resource-metadata.json'\n",
    "    with open(log_path, 'r') as logs:\n",
    "        _logs = json.load(logs)\n",
    "    return _logs['ResourceName']\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f\"SageMaker Execution Role:{role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account:{account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region:{region}\")\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\")\n",
    "if not local_training:\n",
    "    sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "\n",
    "# get default bucket\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print()\n",
    "print(\"Default bucket for this session: \", default_bucket)\n",
    "\n",
    "response = sm_boto_client.describe_notebook_instance(\n",
    "    NotebookInstanceName=get_notebook_name())\n",
    "# set to the number of GPUs on that instance\n",
    "instance_type= response['InstanceType']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This completes the SageMaker setup._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare glue-sst2 data\n",
    "Here you will download, prepare the glue-sst2 dataset and then copy the files to S3. This is done because the `train_gptj_smp_tensor_parallel_script.py` requires either S3 input or paths in an FSx file system of an already tokenized dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries and specify parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, load_from_disk, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import transformers\n",
    "import logging\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "from transformers.testing_utils import CaptureLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"dataset_name\": \"glue\",\n",
    "    \"dataset_config_name\": \"sst2\",\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"cache_dir\": \"tmp\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data\n",
    "This section loads the dataset and splits it to training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset glue (/home/ec2-user/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fa54dbf0bd4aa08e100416f1c21765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\n",
    "    hyperparameters[\"dataset_name\"],\n",
    "    hyperparameters[\"dataset_config_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"validation\" not in raw_datasets.keys():\n",
    "    raw_datasets[\"validation\"] = load_dataset(\n",
    "        hyperparameters[\"dataset_name\"],\n",
    "        hyperparameters[\"dataset_config_name\"],\n",
    "        split=\"train[:5%]\",\n",
    "        cache_dir=hyperparameters[\"cache_dir\"],\n",
    "    )\n",
    "\n",
    "    raw_datasets[\"train\"] = load_dataset(\n",
    "        hyperparameters[\"dataset_name\"],\n",
    "        hyperparameters[\"dataset_config_name\"],\n",
    "        split=\"train[5%:]\",\n",
    "        cache_dir=hyperparameters[\"cache_dir\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load tokenizer\n",
    "Nearly every NLP task begins with a tokenizer. A tokenizer converts your input into a format that can be processed by the model.  \n",
    "The following cell loads a tokenizer with [AutoTokenizer.from_pretrained()](https://huggingface.co/docs/transformers/v4.19.4/en/autoclass_tutorial#autotokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": hyperparameters[\"cache_dir\"],\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\", **tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n",
    "\n",
    "    with CaptureLogger(tok_logger) as cl:\n",
    "        output = tokenizer(examples[text_column_name])\n",
    "        # clm input could be much much longer than block_size\n",
    "        if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "            tok_logger.warning(\n",
    "                \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits before being passed to the model.\"\n",
    "            )\n",
    "    return output\n",
    "\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-09339a23f6473ef0.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bb874275378feb47.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-85ed340213bee516.arrow\n",
      "WARNING:__main__:The tokenizer picked seems to have a very large `model_max_length` (2048). Picking 1024 instead. You can change that default value by passing --block_size xxx.\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-24a7ab3cbc26062b.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-dc4d2f724d89eeb3.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-cc690207792b49b6.arrow\n"
     ]
    }
   ],
   "source": [
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "# since this will be pickled to avoid _LazyModule error in Hasher force logger loading before tokenize_function\n",
    "tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "\n",
    "block_size = tokenizer.model_max_length\n",
    "if block_size > 1024:\n",
    "    logger.warning(\n",
    "        f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "        \"Picking 1024 instead. You can change that default value by passing --block_size xxx.\"\n",
    "    )\n",
    "    block_size = 1024\n",
    "else:\n",
    "    if args.block_size > tokenizer.model_max_length:\n",
    "        logger.warning(\n",
    "            f\"The block_size passed ({block_size}) is larger than the maximum length for the model\"\n",
    "            f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    block_size = min(block_size, tokenizer.model_max_length)\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    #     num_proc=args.preprocessing_num_workers,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparameters[\"do_train\"]:\n",
    "    if \"train\" not in tokenized_datasets:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    train_dataset = lm_datasets[\"train\"]\n",
    "\n",
    "\n",
    "if hyperparameters[\"do_eval\"]:\n",
    "    if \"validation\" not in tokenized_datasets:\n",
    "        raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "    eval_dataset = lm_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a367dd22c44877934b7437296d7217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5996296664404fc0badab3f40a564e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_dataset_location = None\n",
    "validation_dataset_location = None\n",
    "\n",
    "\n",
    "if hyperparameters[\"do_train\"]:\n",
    "    train_dataset.to_json(\"./training.json\")\n",
    "    training_dataset_location = \"s3://{}/dataset/train/\".format(default_bucket)\n",
    "\n",
    "if hyperparameters[\"do_eval\"]:\n",
    "    eval_dataset.to_json(\"./validation.json\")\n",
    "    validation_dataset_location = \"s3://{}/dataset/validation/\".format(default_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_dataset_location is not None:\n",
    "    command = \"aws s3 cp ./training.json {}\".format(training_dataset_location)\n",
    "    os.system(command)\n",
    "\n",
    "if validation_dataset_location is not None:\n",
    "    command = \"aws s3 cp ./validation.json {}\".format(validation_dataset_location)\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparameters[\"do_train\"]:\n",
    "    command = \"rm ./training.json\"\n",
    "    os.system(command)\n",
    "\n",
    "if hyperparameters[\"do_eval\"]:\n",
    "    command = \"rm ./validation.json\"\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'training_dataset_location' (str)\n",
      "Stored 'validation_dataset_location' (str)\n"
     ]
    }
   ],
   "source": [
    "%store training_dataset_location\n",
    "%store validation_dataset_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "local_training                          -> False\n",
      "training_dataset_location               -> 's3://sagemaker-us-west-2-855988369404/dataset/tra\n",
      "validation_dataset_location             -> 's3://sagemaker-us-west-2-855988369404/dataset/val\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Amazon S3 Bucket Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you need to specify the paths for training data to be used by your job. The bucket used must be in the same region as where training will run. In the cells above you downloaded the glue-sst2 training and validation split datasets and uploaded the json files in an S3 bucket in your account. This example will train on those json files.\n",
    "\n",
    "After you successfully run this example tensor parallel training job, you can modify the S3 bucket to where your own dataset is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r training_dataset_location\n",
    "%store -r validation_dataset_location\n",
    "\n",
    "# if you're bringing your own data, uncomment the following lines and specify the locations there\n",
    "# training_dataset_location = YOUR_S3_BUCKET/training\n",
    "# validation_dataset_location = YOUR_S3_BUCKET/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_bucket = training_dataset_location\n",
    "s3_test_bucket = validation_dataset_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below bucket will store output artifacts of the training job. You can modify this as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_bucket = f\"s3://sagemaker-{region}-{account}/smp-tensorparallel-outputdir/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Channels for SageMaker Training\n",
    "\n",
    "In this step, you define SageMaker training data channels using the above buckets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sagemaker.inputs.TrainingInput(\n",
    "        s3_train_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    "    )\n",
    "test = sagemaker.inputs.TrainingInput(\n",
    "        s3_test_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    "    )\n",
    "data_channels = {\"train\": train, \"test\": test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Hyperparameters, Metric Definitions, and MPI Options\n",
    "The following `hyperparameters` dictionary is to pass arguments to the training script (`train_gptj_smp_tesnor_parallel_script.py`) and set the model parallel configuration when creating the training job.\n",
    "\n",
    "You can also add custom mpi flags. By default, we have `--mca btl_vader_single_copy_mechanism none` to remove unnecessary logs.\n",
    "\n",
    "Next we add a base metric definitions to enable the metric upload in SageMaker. You can add any further metric definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_steps\": 50,\n",
    "    \"seed\": 12345,\n",
    "    \"fp16\": 1,\n",
    "    \"lr\": 2.0e-4,\n",
    "    \"lr_decay_iters\": 125000,\n",
    "    \"min_lr\": 0.00001,\n",
    "    \"lr-decay-style\": \"linear\",\n",
    "    \"warmup\": 0.01,\n",
    "    \"num_kept_checkpoints\": 5,\n",
    "    \"checkpoint_freq\": 200,\n",
    "    \"validation_freq\": 1000,\n",
    "    \"logging_freq\": 10,\n",
    "    \"save_final_full_model\": 1,\n",
    "    \"manual_partition\": 0,\n",
    "    \"skip_full_optimizer\": 1,\n",
    "    \"shard_optimizer_state\": 1,\n",
    "    \"activation_checkpointing\": 1,\n",
    "    \"activation_strategy\": \"group_2\",\n",
    "    \"optimize\": \"speed\",\n",
    "    # below flag loads model and optimizer state from checkpoint_s3_uri\n",
    "    # 'load_partial': 1,\n",
    "}\n",
    "\n",
    "mpioptions = \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR \"\n",
    "mpioptions += (\n",
    "    \"-x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 \"\n",
    ")\n",
    "mpioptions += \"-x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\"\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"base_metric\", \"Regex\": \"<><><><><><>\"}\n",
    "]  # Add your custom metric definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the model configuration below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = \"gpt-j-6B\"\n",
    "\n",
    "if model_config == \"gpt-j-6B\":\n",
    "    model_params = {\n",
    "        \"tensor_parallel_degree\": 4,\n",
    "        \"pipeline_parallel_degree\": 1,\n",
    "        \"train_batch_size\": 8,\n",
    "        \"val_batch_size\": 8,\n",
    "        \"prescaled_batch\": 1,\n",
    "    }\n",
    "\n",
    "for k, v in model_params.items():\n",
    "    hyperparameters[k] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up SageMaker Studio Experiment\n",
    "Create or load [SageMaker Experiment](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) for the example training job. This will create an experiment trial object in SageMaker Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "# Specify your experiment name\n",
    "experiment_name = \"smp-gptj-tensor-parallel\"\n",
    "# Specify your trial name\n",
    "trial_name = f\"{experiment_name}-trial1\"\n",
    "\n",
    "all_experiment_names = [exp.experiment_name for exp in Experiment.list()]\n",
    "# Load the experiment if it exists, otherwise create\n",
    "if experiment_name not in all_experiment_names:\n",
    "    experiment = Experiment.create(\n",
    "        experiment_name=experiment_name, sagemaker_boto_client=sm_boto_client\n",
    "    )\n",
    "else:\n",
    "    experiment = Experiment.load(\n",
    "        experiment_name=experiment_name, sagemaker_boto_client=sm_boto_client\n",
    "    )\n",
    "\n",
    "# Create the trial\n",
    "trial = Trial.create(\n",
    "    trial_name=\"smp-{}-{}\".format(trial_name, strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())),\n",
    "    experiment_name=experiment.experiment_name,\n",
    "    sagemaker_boto_client=sm_boto_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Essential Parameters for a SageMaker Training Job\n",
    "\n",
    "Next, you will use the [`SageMaker Estimator API`](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) to define a SageMaker Training Job, passing values through the following parameters for training job name, the number of EC2 instances, the instance type, and the size of the volume attached to the instances. \n",
    "\n",
    "* `instance_count`\n",
    "* `instance_type`\n",
    "* `volume_size`\n",
    "* `base_job_name`\n",
    "\n",
    "### Update the Type of EC2 Instance to Use\n",
    "\n",
    "The instance type and the number of instances you specify to the `instance_type` and `instance_count` parameters, respectively, will determine the total number of GPUs (world size).\n",
    "\n",
    "$$ \\text{(world size) = (the number of GPUs on a single instance)}\\times\\text{(the number of instance)}$$\n",
    "\n",
    "<font color='red'>If you are running training in remote mode and if you get a Training job error due to instance type unavailability, you need to try again by selecting another instance type from the below list in priority order by setting the instance_type in the below cell -\n",
    "\n",
    "<li>ml.g5.12xlarge\n",
    "<li>ml.g5.24xlarge\n",
    "<li>ml.g5.48xlarge\n",
    "<li>ml.g4dn.12xlarge\n",
    "<li>ml.p3.8xlarge\n",
    "<li>ml.p3dn.24xlarge\n",
    "<li>ml.p2.16xlarge\n",
    " </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not local_training:\n",
    "    instance_type = \"ml.g5.48xlarge\"\n",
    "\n",
    "instance_count = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ml.g5.48xlarge'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processes_per_host is set to: 8\n"
     ]
    }
   ],
   "source": [
    "if instance_type in ['ml.p3.16xlarge','p3dn.24xlarge','ml.g5.48xlarge', 'ml.p4d.24xlarge']:\n",
    "    processes_per_host = 8\n",
    "elif instance_type == 'ml.p2.16xlarge':\n",
    "    processes_per_host = 16\n",
    "else:\n",
    "    processes_per_host = 4\n",
    "\n",
    "print(\"processes_per_host is set to:\",processes_per_host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look up the number of GPUs of different instance types, see [Amazon EC2 Instance Types](https://aws.amazon.com/ec2/instance-types/). Use the section **Accelerated Computing** to see general purpose GPU instances. Note that, for example, a given instance type `p4d.24xlarge` has a corresponding instance type `ml.p4d.24xlarge` in SageMaker.\n",
    "For SageMaker supported `ml` instances and cost information, see [Amazon SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach an EBS Volume to the Training Instance\n",
    "The volume size you specify in `volume_size` must be larger than your input data size. In this example, the volume size is set to 500GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_size = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify a Base Job Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "pp_degree = hyperparameters[\"pipeline_parallel_degree\"]\n",
    "tp_degree = hyperparameters[\"tensor_parallel_degree\"]\n",
    "base_job_name = f'smp-{model_config}-{machine_str}-tp{tp_degree}-pp{pp_degree}-bs{hyperparameters[\"train_batch_size\"]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SageMaker HuggingFace 🤗 Estimator\n",
    "\n",
    "The following cell constructs a PyTorch estimator using the parameters defined above. To see how the SageMaker tensor parallelism modules and functions are applied to the script, see the `train_gptj_smp_tensor_parallel_script.py` file and the private preview documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_training:\n",
    "    instance_type = 'local_gpu'\n",
    "        \n",
    "smp_estimator = HuggingFace(\n",
    "    entry_point=\"train_gptj_smp_tensor_parallel_script.py\",\n",
    "    source_dir=os.getcwd(),\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=volume_size,\n",
    "    instance_count=instance_count,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    distribution={\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\": processes_per_host,\n",
    "            \"custom_mpi_options\": mpioptions,\n",
    "        },\n",
    "        \"smdistributed\": {\n",
    "            \"modelparallel\": {\n",
    "                \"enabled\": True,\n",
    "                \"parameters\": {\n",
    "                    \"ddp\": True,\n",
    "                    \"tensor_parallel_degree\": hyperparameters[\"tensor_parallel_degree\"],\n",
    "                    # partitions is a required param in the current SM SDK so it needs to be passed,\n",
    "                    # these two map to the same config\n",
    "                    \"partitions\": hyperparameters[\"pipeline_parallel_degree\"],\n",
    "                    \"shard_optimizer_state\": hyperparameters[\"shard_optimizer_state\"] > 0,\n",
    "                    \"prescaled_batch\": hyperparameters[\"prescaled_batch\"] > 0,\n",
    "                    \"fp16_params\": hyperparameters[\"fp16\"] > 0,\n",
    "                    \"optimize\": hyperparameters[\"optimize\"],\n",
    "                    \"auto_partition\": False if hyperparameters[\"manual_partition\"] else True,\n",
    "                    \"default_partition\": 0,\n",
    "                    \"fp16_params\": hyperparameters[\"fp16\"] > 0,\n",
    "                    \"optimize\": hyperparameters[\"optimize\"],\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    pytorch_version=\"1.10\",\n",
    "    transformers_version=\"4.17\",\n",
    "    py_version=\"py38\",\n",
    "    output_path=s3_output_bucket,\n",
    "#     checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    metric_definitions=metric_definitions,\n",
    "    hyperparameters=hyperparameters,\n",
    "    debugger_hook_config=False,\n",
    "    disable_profiler=True,\n",
    "    base_job_name=base_job_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the estimator to launch the SageMaker training job of GPT-J model with tensor parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you receive a `ResourceLimitExceeded` error message when running the following cell, you can request an increase on the default quota by contacting [AWS support](https://console.aws.amazon.com/support). Open the [AWS Support Center](https://console.aws.amazon.com/support), and then choose Create case. Choose Service limit increase. For Limit Type choose SageMaker Training Jobs. Complete the rest of the form and submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: smp-gpt-j-6B-g548x-tp4-pp1-bs8-2022-06-22-20-09-49-244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-22 20:10:22 Starting - Starting the training job......\n",
      "2022-06-22 20:10:59 Starting - Preparing the instances for training.........\n",
      "2022-06-22 20:12:41 Downloading - Downloading input data...\n",
      "2022-06-22 20:12:57 Training - Downloading the training image.....................\n",
      "2022-06-22 20:16:38 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/paramiko/transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\u001b[0m\n",
      "\u001b[34m2022-06-22 20:16:41,166 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-06-22 20:16:41,241 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-06-22 20:16:41,247 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-06-22 20:16:45,498 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.18.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.22.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (2.89.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker-experiments in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (0.1.35)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchnet in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (0.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (4.17.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug in /opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg (from -r requirements.txt (line 8)) (1.0.13b20220512)\u001b[0m\n",
      "\u001b[34mCollecting humanize\u001b[0m\n",
      "\u001b[34mDownloading humanize-4.2.0-py3-none-any.whl (102 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.5/102.5 kB 5.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting smart-open>=5.2.1\u001b[0m\n",
      "\u001b[34mDownloading smart_open-6.0.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.4/58.4 kB 12.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3>=1.23 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (1.26.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (2022.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (8.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (2.27.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (4.63.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (1.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (3.20.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3>=1.20.21 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.22.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs==20.3.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (20.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (4.11.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from torchnet->-r requirements.txt (line 6)) (1.10.2+cu113)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: visdom in /opt/conda/lib/python3.8/site-packages (from torchnet->-r requirements.txt (line 6)) (0.1.8.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from torchnet->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 7)) (2022.4.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 7)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 7)) (0.0.53)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 7)) (3.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 7)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyinstrument==3.4.2 in /opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg (from smdebug->-r requirements.txt (line 8)) (3.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyinstrument-cext>=0.2.2 in /opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg (from pyinstrument==3.4.2->smdebug->-r requirements.txt (line 8)) (0.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.26.0,>=1.25.13 in /opt/conda/lib/python3.8/site-packages (from boto3>=1.20.21->sagemaker->-r requirements.txt (line 3)) (1.25.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3>=1.20.21->sagemaker->-r requirements.txt (line 3)) (1.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from boto3>=1.20.21->sagemaker->-r requirements.txt (line 3)) (0.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->-r requirements.txt (line 1)) (4.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=1.4.0->sagemaker->-r requirements.txt (line 3)) (3.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets->-r requirements.txt (line 1)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (2.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (2021.10.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2022.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (1.6.6.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers->-r requirements.txt (line 7)) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers->-r requirements.txt (line 7)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyzmq in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (22.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tornado in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (9.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: websocket-client in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (1.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchfile in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (0.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonpatch in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (1.32)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.8/site-packages (from jsonpatch->visdom->torchnet->-r requirements.txt (line 6)) (2.3)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: smart-open, humanize\u001b[0m\n",
      "\u001b[34mSuccessfully installed humanize-4.2.0 smart-open-6.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mWARNING: There was an error checking the latest version of pip.\u001b[0m\n",
      "\u001b[34m2022-06-22 20:16:48,632 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2022-06-22 20:16:48,632 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2022-06-22 20:16:48,636 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2022-06-22 20:16:48,636 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1:8'] process_per_hosts: 8 num_processes: 8\u001b[0m\n",
      "\u001b[34m2022-06-22 20:16:48,637 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2022-06-22 20:16:48,749 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_instance_type\": \"ml.g5.48xlarge\",\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 8\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"activation_checkpointing\": 1,\n",
      "        \"activation_strategy\": \"group_2\",\n",
      "        \"checkpoint_freq\": 200,\n",
      "        \"fp16\": 1,\n",
      "        \"logging_freq\": 10,\n",
      "        \"lr\": 0.0002,\n",
      "        \"lr-decay-style\": \"linear\",\n",
      "        \"lr_decay_iters\": 125000,\n",
      "        \"manual_partition\": 0,\n",
      "        \"max_steps\": 50,\n",
      "        \"min_lr\": 1e-05,\n",
      "        \"mp_parameters\": {\n",
      "            \"ddp\": true,\n",
      "            \"tensor_parallel_degree\": 4,\n",
      "            \"partitions\": 1,\n",
      "            \"shard_optimizer_state\": true,\n",
      "            \"prescaled_batch\": true,\n",
      "            \"fp16_params\": true,\n",
      "            \"optimize\": \"speed\",\n",
      "            \"auto_partition\": true,\n",
      "            \"default_partition\": 0\n",
      "        },\n",
      "        \"num_kept_checkpoints\": 5,\n",
      "        \"optimize\": \"speed\",\n",
      "        \"pipeline_parallel_degree\": 1,\n",
      "        \"prescaled_batch\": 1,\n",
      "        \"save_final_full_model\": 1,\n",
      "        \"seed\": 12345,\n",
      "        \"shard_optimizer_state\": 1,\n",
      "        \"skip_full_optimizer\": 1,\n",
      "        \"tensor_parallel_degree\": 4,\n",
      "        \"train_batch_size\": 8,\n",
      "        \"val_batch_size\": 8,\n",
      "        \"validation_freq\": 1000,\n",
      "        \"warmup\": 0.01\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"smp-gpt-j-6B-g548x-tp4-pp1-bs8-2022-06-22-20-09-49-244\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-855988369404/smp-gpt-j-6B-g548x-tp4-pp1-bs8-2022-06-22-20-09-49-244/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_gptj_smp_tensor_parallel_script\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 192,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.48xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_gptj_smp_tensor_parallel_script.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"activation_checkpointing\":1,\"activation_strategy\":\"group_2\",\"checkpoint_freq\":200,\"fp16\":1,\"logging_freq\":10,\"lr\":0.0002,\"lr-decay-style\":\"linear\",\"lr_decay_iters\":125000,\"manual_partition\":0,\"max_steps\":50,\"min_lr\":1e-05,\"mp_parameters\":{\"auto_partition\":true,\"ddp\":true,\"default_partition\":0,\"fp16_params\":true,\"optimize\":\"speed\",\"partitions\":1,\"prescaled_batch\":true,\"shard_optimizer_state\":true,\"tensor_parallel_degree\":4},\"num_kept_checkpoints\":5,\"optimize\":\"speed\",\"pipeline_parallel_degree\":1,\"prescaled_batch\":1,\"save_final_full_model\":1,\"seed\":12345,\"shard_optimizer_state\":1,\"skip_full_optimizer\":1,\"tensor_parallel_degree\":4,\"train_batch_size\":8,\"val_batch_size\":8,\"validation_freq\":1000,\"warmup\":0.01}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_gptj_smp_tensor_parallel_script.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_gptj_smp_tensor_parallel_script\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=192\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-855988369404/smp-gpt-j-6B-g548x-tp4-pp1-bs8-2022-06-22-20-09-49-244/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.g5.48xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"activation_checkpointing\":1,\"activation_strategy\":\"group_2\",\"checkpoint_freq\":200,\"fp16\":1,\"logging_freq\":10,\"lr\":0.0002,\"lr-decay-style\":\"linear\",\"lr_decay_iters\":125000,\"manual_partition\":0,\"max_steps\":50,\"min_lr\":1e-05,\"mp_parameters\":{\"auto_partition\":true,\"ddp\":true,\"default_partition\":0,\"fp16_params\":true,\"optimize\":\"speed\",\"partitions\":1,\"prescaled_batch\":true,\"shard_optimizer_state\":true,\"tensor_parallel_degree\":4},\"num_kept_checkpoints\":5,\"optimize\":\"speed\",\"pipeline_parallel_degree\":1,\"prescaled_batch\":1,\"save_final_full_model\":1,\"seed\":12345,\"shard_optimizer_state\":1,\"skip_full_optimizer\":1,\"tensor_parallel_degree\":4,\"train_batch_size\":8,\"val_batch_size\":8,\"validation_freq\":1000,\"warmup\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"smp-gpt-j-6B-g548x-tp4-pp1-bs8-2022-06-22-20-09-49-244\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-855988369404/smp-gpt-j-6B-g548x-tp4-pp1-bs8-2022-06-22-20-09-49-244/source/sourcedir.tar.gz\",\"module_name\":\"train_gptj_smp_tensor_parallel_script\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_gptj_smp_tensor_parallel_script.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--activation_checkpointing\",\"1\",\"--activation_strategy\",\"group_2\",\"--checkpoint_freq\",\"200\",\"--fp16\",\"1\",\"--logging_freq\",\"10\",\"--lr\",\"0.0002\",\"--lr-decay-style\",\"linear\",\"--lr_decay_iters\",\"125000\",\"--manual_partition\",\"0\",\"--max_steps\",\"50\",\"--min_lr\",\"1e-05\",\"--mp_parameters\",\"auto_partition=True,ddp=True,default_partition=0,fp16_params=True,optimize=speed,partitions=1,prescaled_batch=True,shard_optimizer_state=True,tensor_parallel_degree=4\",\"--num_kept_checkpoints\",\"5\",\"--optimize\",\"speed\",\"--pipeline_parallel_degree\",\"1\",\"--prescaled_batch\",\"1\",\"--save_final_full_model\",\"1\",\"--seed\",\"12345\",\"--shard_optimizer_state\",\"1\",\"--skip_full_optimizer\",\"1\",\"--tensor_parallel_degree\",\"4\",\"--train_batch_size\",\"8\",\"--val_batch_size\",\"8\",\"--validation_freq\",\"1000\",\"--warmup\",\"0.01\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_ACTIVATION_CHECKPOINTING=1\u001b[0m\n",
      "\u001b[34mSM_HP_ACTIVATION_STRATEGY=group_2\u001b[0m\n",
      "\u001b[34mSM_HP_CHECKPOINT_FREQ=200\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=1\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_FREQ=10\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LR-DECAY-STYLE=linear\u001b[0m\n",
      "\u001b[34mSM_HP_LR_DECAY_ITERS=125000\u001b[0m\n",
      "\u001b[34mSM_HP_MANUAL_PARTITION=0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=50\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_LR=1e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MP_PARAMETERS={\"auto_partition\":true,\"ddp\":true,\"default_partition\":0,\"fp16_params\":true,\"optimize\":\"speed\",\"partitions\":1,\"prescaled_batch\":true,\"shard_optimizer_state\":true,\"tensor_parallel_degree\":4}\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_KEPT_CHECKPOINTS=5\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZE=speed\u001b[0m\n",
      "\u001b[34mSM_HP_PIPELINE_PARALLEL_DEGREE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PRESCALED_BATCH=1\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_FINAL_FULL_MODEL=1\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=12345\u001b[0m\n",
      "\u001b[34mSM_HP_SHARD_OPTIMIZER_STATE=1\u001b[0m\n",
      "\u001b[34mSM_HP_SKIP_FULL_OPTIMIZER=1\u001b[0m\n",
      "\u001b[34mSM_HP_TENSOR_PARALLEL_DEGREE=4\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_VAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_FREQ=1000\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP=0.01\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TEST -x SM_CHANNEL_TRAIN -x SM_HP_ACTIVATION_CHECKPOINTING -x SM_HP_ACTIVATION_STRATEGY -x SM_HP_CHECKPOINT_FREQ -x SM_HP_FP16 -x SM_HP_LOGGING_FREQ -x SM_HP_LR -x SM_HP_LR-DECAY-STYLE -x SM_HP_LR_DECAY_ITERS -x SM_HP_MANUAL_PARTITION -x SM_HP_MAX_STEPS -x SM_HP_MIN_LR -x SM_HP_MP_PARAMETERS -x SM_HP_NUM_KEPT_CHECKPOINTS -x SM_HP_OPTIMIZE -x SM_HP_PIPELINE_PARALLEL_DEGREE -x SM_HP_PRESCALED_BATCH -x SM_HP_SAVE_FINAL_FULL_MODEL -x SM_HP_SEED -x SM_HP_SHARD_OPTIMIZER_STATE -x SM_HP_SKIP_FULL_OPTIMIZER -x SM_HP_TENSOR_PARALLEL_DEGREE -x SM_HP_TRAIN_BATCH_SIZE -x SM_HP_VAL_BATCH_SIZE -x SM_HP_VALIDATION_FREQ -x SM_HP_WARMUP -x PYTHONPATH /opt/conda/bin/python3.8 -m mpi4py train_gptj_smp_tensor_parallel_script.py --activation_checkpointing 1 --activation_strategy group_2 --checkpoint_freq 200 --fp16 1 --logging_freq 10 --lr 0.0002 --lr-decay-style linear --lr_decay_iters 125000 --manual_partition 0 --max_steps 50 --min_lr 1e-05 --mp_parameters auto_partition=True,ddp=True,default_partition=0,fp16_params=True,optimize=speed,partitions=1,prescaled_batch=True,shard_optimizer_state=True,tensor_parallel_degree=4 --num_kept_checkpoints 5 --optimize speed --pipeline_parallel_degree 1 --prescaled_batch 1 --save_final_full_model 1 --seed 12345 --shard_optimizer_state 1 --skip_full_optimizer 1 --tensor_parallel_degree 4 --train_batch_size 8 --val_batch_size 8 --validation_freq 1000 --warmup 0.01\u001b[0m\n",
      "\u001b[34m[algo-1:00046] Warning: could not find environment variable \"SM_HP_LR-DECAY-STYLE\"\u001b[0m\n",
      "\u001b[34mData for JOB [41160,1] offset 0 Total slots allocated 8\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: algo-1#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41160,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41160,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [41160,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41160,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [41160,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [41160,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [41160,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [41160,1] App: 0 Process rank: 7 Bound: N/A\n",
      " =============================================================\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.500: I smdistributed/modelparallel/torch/state_mod.py:162] [0] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.500: I smdistributed/modelparallel/torch/throttler.py:37] Using NCCL throttle limit of 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-22 20:16:57.500: I smdistributed/modelparallel/torch/state_mod.py:162] [3] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 3, dp_rank: 3, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-22 20:16:57.500: I smdistributed/modelparallel/torch/state_mod.py:162] [1] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 1, dp_rank: 1, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-22 20:16:57.500: I smdistributed/modelparallel/torch/state_mod.py:162] [2] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 2, dp_rank: 2, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-22 20:16:57.501: I smdistributed/modelparallel/torch/state_mod.py:162] [5] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 1, dp_rank: 5, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-22 20:16:57.501: I smdistributed/modelparallel/torch/state_mod.py:162] [6] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 2, dp_rank: 6, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-22 20:16:57.501: I smdistributed/modelparallel/torch/state_mod.py:162] [7] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 3, dp_rank: 7, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-22 20:16:57.510: I smdistributed/modelparallel/torch/state_mod.py:162] [4] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 4, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.681: I smdistributed/modelparallel/backend/config.py:234] Configuration parameters:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.681: I smdistributed/modelparallel/backend/config.py:237]   pipeline_parallel_degree: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.681: I smdistributed/modelparallel/backend/config.py:237]   microbatches: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.681: I smdistributed/modelparallel/backend/config.py:237]   pipeline: interleaved\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.681: I smdistributed/modelparallel/backend/config.py:237]   horovod: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.682: I smdistributed/modelparallel/backend/config.py:237]   ddp: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.682: I smdistributed/modelparallel/backend/config.py:237]   tensor_parallel_degree: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.682: I smdistributed/modelparallel/backend/config.py:237]   ddp_port: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.682: I smdistributed/modelparallel/backend/config.py:237]   ddp_dist_backend: nccl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.682: I smdistributed/modelparallel/backend/config.py:237]   contiguous: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.682: I smdistributed/modelparallel/backend/config.py:237]   placement_strategy: cluster\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.682: I smdistributed/modelparallel/backend/config.py:237]   optimize: speed\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.682: I smdistributed/modelparallel/backend/config.py:237]   default_partition: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.683: I smdistributed/modelparallel/backend/config.py:237]   auto_partition: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.683: I smdistributed/modelparallel/backend/config.py:237]   prescaled_batch: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.683: I smdistributed/modelparallel/backend/config.py:237]   memory_weight: 0.8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.683: I smdistributed/modelparallel/backend/config.py:237]   active_microbatches: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.683: I smdistributed/modelparallel/backend/config.py:237]   fp16_params: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.683: I smdistributed/modelparallel/backend/config.py:237]   tensor_parallel_seed: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.683: I smdistributed/modelparallel/backend/config.py:237]   offload_activations: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.683: I smdistributed/modelparallel/backend/config.py:237]   shard_optimizer_state: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.683: I smdistributed/modelparallel/backend/config.py:237]   skip_tracing: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-22 20:16:57.684: I smdistributed/modelparallel/backend/config.py:237]   activation_loading_horizon: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Arguments:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:{'train_batch_size': 8, 'val_batch_size': 8, 'max_steps': 50, 'seed': 12345, 'same_seed': 0, 'n_gpus': '8', 'fp16': 1, 'fp32_grad_accumulation': 0, 'megatron': 0, 'grad_clip': 1.0, 'weight_decay': 0.01, 'beta1': 0.9, 'beta2': 0.95, 'activation_checkpointing': 1, 'logging_freq': 10, 'use_bert_data': 0, 'zipped_data': 0, 'epochs': 10, 'output_data_dir': '/opt/ml/output/data', 'checkpoint_dir': '/opt/ml/checkpoints', 'model_dir': '/opt/ml/model', 'training_dir': '/opt/ml/input/data/train', 'test_dir': '/opt/ml/input/data/test', 'parallel_proc_data_processing': 0, 'save_final_full_model': 1, 'skip_full_optimizer': 1, 'load_partial': 0, 'load_full': 0, 'logits_output': '', 'prescaled_batch': 1, 'max_context_width': 1024, 'use_adamw': 0, 'tensor_parallel_degree': 4, 'pipeline_parallel_degree': 1, 'microbatches': 1, 'active_microbatches': None, 'optimize': 'speed', 'activation_strategy': 'group_2', 'shard_optimizer_state': 1, 'offload_activations': 0, 'fast_mode': 0, 'static_mode': 0, 'delayed_param': 0, 'same_partition_load': 0, 'attention_in_fp32': 0, 'placement_strategy': 'cluster', 'activation_loading_horizon': 4, 'skip_tracing': 0, 'query_key_layer_scaling': 1, 'fused_softmax': 1, 'fused_bias_gelu': 1, 'num_kept_checkpoints': 5, 'checkpoint_freq': 200, 'validation_freq': 1000, 'validation_batches': 10, 'manual_partition': 0, 'partition_assignment': '', 'match_weights': 0, 'preserve_np_state': 0, 'fast_validation': 1, 'gather_if_shard': 1, 'clean_cache': 0, 'use_fsx': 0, 'enable_memory_profiling': 0, 'lr': 0.0002, 'lr_decay_style': 'linear', 'lr_decay_iters': 125000, 'min_lr': 1e-05, 'warmup': 0.01, 'plateau': 0.4, 'ci': False, 'time_to_train': None, 'throughput': None, 'loss': None, 'save_or_verify_ckptsum': False}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Transformers version: 4.17.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:smdistributed.modelparallel version: 1.8.1[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:smdistributed config: {'ddp': True, 'tensor_parallel_degree': 4, 'pipeline_parallel_degree': 1, 'microbatches': 1, 'checkpoint_attentions': False, 'shard_optimizer_state': True, 'prescaled_batch': True, '_match_weights': False, 'fp16_params': True, 'offload_activations': False, 'optimize': 'speed', 'placement_strategy': 'cluster', 'activation_loading_horizon': 4, 'skip_tracing': False, 'auto_partition': True, 'default_partition': 0, '_fp32_grad_accumulation': False, 'static_mode': False, 'fast_mode': False}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[Warning] Note that save_final_full_model only saves the final model at the end of all steps. It does not save optimizer state. Optimizer state is only saved with partial models which are saved at checkpointing_freq during training. If you want to restart training you need partial checkpoints.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/836 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading: 100%|██████████| 836/836 [00:00<00:00, 1.84MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/11.3G [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   0%|          | 6.60M/11.3G [00:00<02:54, 69.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   0%|          | 15.0M/11.3G [00:00<02:30, 80.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   0%|          | 23.5M/11.3G [00:00<02:23, 84.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   0%|          | 31.6M/11.3G [00:00<03:05, 65.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   0%|          | 39.4M/11.3G [00:00<02:52, 70.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   0%|          | 47.3M/11.3G [00:00<02:42, 74.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   0%|          | 55.1M/11.3G [00:00<02:37, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   1%|          | 62.8M/11.3G [00:00<02:35, 77.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   1%|          | 70.8M/11.3G [00:00<02:31, 79.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   1%|          | 78.6M/11.3G [00:01<03:04, 65.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   1%|          | 86.4M/11.3G [00:01<02:52, 69.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   1%|          | 94.4M/11.3G [00:01<02:43, 73.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   1%|          | 102M/11.3G [00:01<02:43, 73.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   1%|          | 110M/11.3G [00:01<02:35, 77.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   1%|          | 118M/11.3G [00:01<02:39, 74.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   1%|          | 126M/11.3G [00:01<02:33, 77.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   1%|          | 134M/11.3G [00:01<02:29, 80.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   1%|          | 141M/11.3G [00:01<02:31, 78.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   1%|▏         | 149M/11.3G [00:02<02:36, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   1%|▏         | 157M/11.3G [00:02<02:35, 77.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   1%|▏         | 164M/11.3G [00:02<02:38, 75.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   1%|▏         | 172M/11.3G [00:02<02:32, 78.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   2%|▏         | 180M/11.3G [00:02<02:29, 79.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   2%|▏         | 188M/11.3G [00:02<02:25, 81.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   2%|▏         | 196M/11.3G [00:02<02:37, 75.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   2%|▏         | 204M/11.3G [00:02<02:35, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   2%|▏         | 211M/11.3G [00:02<02:54, 68.2MB/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   2%|▏         | 220M/11.3G [00:03<02:40, 74.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   2%|▏         | 227M/11.3G [00:03<02:45, 71.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   2%|▏         | 234M/11.3G [00:03<02:47, 71.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   2%|▏         | 241M/11.3G [00:03<02:55, 67.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   2%|▏         | 249M/11.3G [00:03<02:44, 71.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   2%|▏         | 256M/11.3G [00:03<02:42, 73.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   2%|▏         | 263M/11.3G [00:03<02:43, 72.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   2%|▏         | 271M/11.3G [00:03<02:37, 75.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   2%|▏         | 279M/11.3G [00:03<02:32, 77.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   2%|▏         | 287M/11.3G [00:04<02:28, 79.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   3%|▎         | 295M/11.3G [00:04<02:25, 81.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   3%|▎         | 303M/11.3G [00:04<02:23, 81.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   3%|▎         | 311M/11.3G [00:04<02:26, 80.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   3%|▎         | 319M/11.3G [00:04<02:27, 80.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   3%|▎         | 326M/11.3G [00:04<02:35, 75.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   3%|▎         | 333M/11.3G [00:04<03:40, 53.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   3%|▎         | 340M/11.3G [00:04<03:25, 57.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   3%|▎         | 348M/11.3G [00:04<03:06, 62.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   3%|▎         | 356M/11.3G [00:05<02:53, 67.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   3%|▎         | 363M/11.3G [00:05<02:55, 66.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   3%|▎         | 369M/11.3G [00:05<03:13, 60.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   3%|▎         | 376M/11.3G [00:05<03:42, 52.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   3%|▎         | 382M/11.3G [00:05<03:24, 57.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   3%|▎         | 390M/11.3G [00:05<03:08, 62.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   3%|▎         | 398M/11.3G [00:05<02:52, 67.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   4%|▎         | 405M/11.3G [00:05<02:50, 68.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   4%|▎         | 413M/11.3G [00:06<02:37, 74.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   4%|▎         | 421M/11.3G [00:06<02:30, 77.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   4%|▎         | 429M/11.3G [00:06<02:25, 80.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   4%|▍         | 437M/11.3G [00:06<02:32, 76.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   4%|▍         | 445M/11.3G [00:06<02:32, 76.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   4%|▍         | 453M/11.3G [00:06<02:27, 78.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   4%|▍         | 461M/11.3G [00:06<02:23, 81.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   4%|▍         | 469M/11.3G [00:06<02:22, 81.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   4%|▍         | 477M/11.3G [00:06<02:25, 80.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   4%|▍         | 485M/11.3G [00:06<02:23, 81.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   4%|▍         | 493M/11.3G [00:07<02:40, 72.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   4%|▍         | 500M/11.3G [00:07<03:10, 60.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   4%|▍         | 506M/11.3G [00:07<03:04, 62.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   4%|▍         | 514M/11.3G [00:07<02:48, 68.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   5%|▍         | 523M/11.3G [00:07<02:37, 73.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   5%|▍         | 531M/11.3G [00:07<02:29, 77.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   5%|▍         | 539M/11.3G [00:07<02:24, 79.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   5%|▍         | 547M/11.3G [00:07<02:24, 80.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   5%|▍         | 555M/11.3G [00:07<02:20, 81.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   5%|▍         | 563M/11.3G [00:08<02:20, 82.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   5%|▍         | 571M/11.3G [00:08<02:17, 83.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   5%|▌         | 579M/11.3G [00:08<02:17, 83.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   5%|▌         | 587M/11.3G [00:08<02:20, 82.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   5%|▌         | 595M/11.3G [00:08<02:27, 77.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   5%|▌         | 603M/11.3G [00:08<02:30, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   5%|▌         | 611M/11.3G [00:08<02:28, 77.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   5%|▌         | 619M/11.3G [00:08<02:26, 78.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   5%|▌         | 626M/11.3G [00:08<02:24, 79.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   5%|▌         | 634M/11.3G [00:09<02:40, 71.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   6%|▌         | 641M/11.3G [00:09<02:35, 73.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   6%|▌         | 649M/11.3G [00:09<02:29, 76.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   6%|▌         | 658M/11.3G [00:09<02:23, 79.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   6%|▌         | 666M/11.3G [00:09<02:20, 81.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   6%|▌         | 674M/11.3G [00:09<02:17, 82.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   6%|▌         | 682M/11.3G [00:09<02:20, 81.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   6%|▌         | 690M/11.3G [00:09<02:22, 80.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   6%|▌         | 698M/11.3G [00:09<02:52, 66.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   6%|▌         | 704M/11.3G [00:10<02:47, 67.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   6%|▌         | 712M/11.3G [00:10<02:38, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   6%|▌         | 720M/11.3G [00:10<02:35, 73.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   6%|▋         | 728M/11.3G [00:10<02:26, 77.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   6%|▋         | 736M/11.3G [00:10<02:20, 80.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   6%|▋         | 745M/11.3G [00:10<02:17, 82.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   7%|▋         | 753M/11.3G [00:10<02:33, 73.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   7%|▋         | 760M/11.3G [00:10<02:30, 75.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   7%|▋         | 768M/11.3G [00:10<02:26, 77.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   7%|▋         | 776M/11.3G [00:10<02:23, 78.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   7%|▋         | 784M/11.3G [00:11<02:19, 81.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   7%|▋         | 792M/11.3G [00:11<02:16, 82.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   7%|▋         | 800M/11.3G [00:11<02:29, 75.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   7%|▋         | 808M/11.3G [00:11<02:43, 68.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   7%|▋         | 814M/11.3G [00:11<03:13, 58.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   7%|▋         | 822M/11.3G [00:11<02:57, 63.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   7%|▋         | 829M/11.3G [00:11<03:21, 55.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   7%|▋         | 835M/11.3G [00:11<03:14, 57.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   7%|▋         | 842M/11.3G [00:12<02:55, 63.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   7%|▋         | 850M/11.3G [00:12<02:43, 68.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   7%|▋         | 857M/11.3G [00:12<02:44, 68.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   7%|▋         | 864M/11.3G [00:12<02:43, 68.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   8%|▊         | 871M/11.3G [00:12<02:39, 70.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   8%|▊         | 879M/11.3G [00:12<02:29, 74.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   8%|▊         | 888M/11.3G [00:12<02:21, 78.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   8%|▊         | 895M/11.3G [00:12<02:20, 79.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   8%|▊         | 903M/11.3G [00:12<02:54, 63.9MB/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   8%|▊         | 910M/11.3G [00:13<02:53, 64.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   8%|▊         | 918M/11.3G [00:13<02:39, 69.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   8%|▊         | 925M/11.3G [00:13<02:33, 72.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   8%|▊         | 933M/11.3G [00:13<02:26, 76.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   8%|▊         | 941M/11.3G [00:13<02:22, 78.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   8%|▊         | 949M/11.3G [00:13<02:23, 77.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   8%|▊         | 957M/11.3G [00:13<02:19, 79.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   8%|▊         | 965M/11.3G [00:13<02:17, 80.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   8%|▊         | 973M/11.3G [00:13<02:14, 82.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   8%|▊         | 981M/11.3G [00:13<02:19, 79.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   9%|▊         | 989M/11.3G [00:14<02:23, 77.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   9%|▊         | 996M/11.3G [00:14<02:30, 73.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   9%|▊         | 0.98G/11.3G [00:14<02:32, 72.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   9%|▉         | 0.99G/11.3G [00:14<02:27, 74.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   9%|▉         | 1.00G/11.3G [00:14<02:22, 77.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   9%|▉         | 1.00G/11.3G [00:14<02:27, 75.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   9%|▉         | 1.01G/11.3G [00:14<02:28, 74.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   9%|▉         | 1.02G/11.3G [00:14<02:26, 75.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   9%|▉         | 1.02G/11.3G [00:14<02:43, 67.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   9%|▉         | 1.03G/11.3G [00:15<04:13, 43.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   9%|▉         | 1.04G/11.3G [00:15<03:31, 52.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   9%|▉         | 1.05G/11.3G [00:15<03:02, 60.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   9%|▉         | 1.06G/11.3G [00:15<02:43, 67.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   9%|▉         | 1.06G/11.3G [00:15<02:29, 73.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  10%|▉         | 1.07G/11.3G [00:15<02:22, 76.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  10%|▉         | 1.08G/11.3G [00:15<02:19, 78.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  10%|▉         | 1.09G/11.3G [00:15<02:17, 79.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  10%|▉         | 1.09G/11.3G [00:16<02:16, 79.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  10%|▉         | 1.10G/11.3G [00:16<02:16, 80.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  10%|▉         | 1.11G/11.3G [00:16<02:14, 81.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  10%|▉         | 1.12G/11.3G [00:16<02:15, 80.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  10%|▉         | 1.13G/11.3G [00:16<02:14, 81.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  10%|█         | 1.13G/11.3G [00:16<02:14, 80.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  10%|█         | 1.14G/11.3G [00:16<02:13, 81.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  10%|█         | 1.15G/11.3G [00:16<02:15, 80.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  10%|█         | 1.16G/11.3G [00:16<02:20, 77.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  10%|█         | 1.16G/11.3G [00:17<02:28, 72.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  10%|█         | 1.17G/11.3G [00:17<03:41, 48.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  10%|█         | 1.18G/11.3G [00:17<03:26, 52.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  10%|█         | 1.18G/11.3G [00:17<03:22, 53.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  11%|█         | 1.19G/11.3G [00:17<02:56, 61.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  11%|█         | 1.20G/11.3G [00:17<02:40, 67.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  11%|█         | 1.21G/11.3G [00:17<02:30, 71.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  11%|█         | 1.21G/11.3G [00:17<02:34, 69.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  11%|█         | 1.22G/11.3G [00:18<02:29, 72.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  11%|█         | 1.23G/11.3G [00:18<02:23, 75.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  11%|█         | 1.23G/11.3G [00:18<02:27, 73.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  11%|█         | 1.24G/11.3G [00:18<02:37, 68.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  11%|█         | 1.25G/11.3G [00:18<02:44, 65.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  11%|█         | 1.26G/11.3G [00:18<02:35, 69.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  11%|█         | 1.26G/11.3G [00:18<02:27, 73.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  11%|█▏        | 1.27G/11.3G [00:18<02:25, 73.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  11%|█▏        | 1.28G/11.3G [00:18<02:21, 75.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  11%|█▏        | 1.29G/11.3G [00:19<02:17, 78.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  11%|█▏        | 1.29G/11.3G [00:19<02:15, 79.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  12%|█▏        | 1.30G/11.3G [00:19<02:18, 77.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  12%|█▏        | 1.31G/11.3G [00:19<02:24, 74.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  12%|█▏        | 1.32G/11.3G [00:19<02:17, 77.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  12%|█▏        | 1.32G/11.3G [00:19<02:16, 78.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  12%|█▏        | 1.33G/11.3G [00:19<02:14, 79.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  12%|█▏        | 1.34G/11.3G [00:19<02:20, 76.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  12%|█▏        | 1.35G/11.3G [00:19<02:18, 77.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  12%|█▏        | 1.35G/11.3G [00:19<02:15, 78.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  12%|█▏        | 1.36G/11.3G [00:20<02:14, 79.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  12%|█▏        | 1.37G/11.3G [00:20<02:11, 81.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  12%|█▏        | 1.38G/11.3G [00:20<02:25, 73.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  12%|█▏        | 1.38G/11.3G [00:20<02:22, 74.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  12%|█▏        | 1.39G/11.3G [00:20<02:21, 75.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  12%|█▏        | 1.40G/11.3G [00:20<02:21, 75.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  12%|█▏        | 1.41G/11.3G [00:20<02:18, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  13%|█▎        | 1.41G/11.3G [00:20<02:15, 78.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  13%|█▎        | 1.42G/11.3G [00:20<02:19, 75.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  13%|█▎        | 1.43G/11.3G [00:20<02:17, 76.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  13%|█▎        | 1.44G/11.3G [00:21<02:14, 78.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  13%|█▎        | 1.44G/11.3G [00:21<02:25, 72.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  13%|█▎        | 1.45G/11.3G [00:21<02:21, 74.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  13%|█▎        | 1.46G/11.3G [00:21<02:15, 77.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  13%|█▎        | 1.47G/11.3G [00:21<02:23, 73.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  13%|█▎        | 1.47G/11.3G [00:21<02:36, 67.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  13%|█▎        | 1.48G/11.3G [00:21<02:34, 68.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  13%|█▎        | 1.49G/11.3G [00:21<02:35, 67.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  13%|█▎        | 1.49G/11.3G [00:21<02:35, 67.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  13%|█▎        | 1.50G/11.3G [00:22<02:27, 71.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  13%|█▎        | 1.51G/11.3G [00:22<02:19, 75.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  13%|█▎        | 1.51G/11.3G [00:22<02:24, 72.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  13%|█▎        | 1.52G/11.3G [00:22<02:21, 74.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  14%|█▎        | 1.53G/11.3G [00:22<02:20, 74.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  14%|█▎        | 1.54G/11.3G [00:22<02:16, 76.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  14%|█▎        | 1.54G/11.3G [00:22<02:18, 75.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  14%|█▍        | 1.55G/11.3G [00:22<02:17, 76.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  14%|█▍        | 1.56G/11.3G [00:22<02:17, 76.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  14%|█▍        | 1.57G/11.3G [00:22<02:12, 78.9MB/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  14%|█▍        | 1.57G/11.3G [00:23<02:09, 80.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  14%|█▍        | 1.58G/11.3G [00:23<02:06, 82.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  14%|█▍        | 1.59G/11.3G [00:23<02:24, 71.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  14%|█▍        | 1.60G/11.3G [00:23<02:25, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  14%|█▍        | 1.60G/11.3G [00:23<02:22, 72.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  14%|█▍        | 1.61G/11.3G [00:23<02:18, 74.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  14%|█▍        | 1.62G/11.3G [00:23<02:18, 74.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  14%|█▍        | 1.63G/11.3G [00:23<02:15, 76.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  14%|█▍        | 1.63G/11.3G [00:23<02:13, 77.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  15%|█▍        | 1.64G/11.3G [00:24<02:09, 80.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  15%|█▍        | 1.65G/11.3G [00:24<02:07, 81.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  15%|█▍        | 1.66G/11.3G [00:24<02:06, 81.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  15%|█▍        | 1.66G/11.3G [00:24<02:05, 82.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  15%|█▍        | 1.67G/11.3G [00:24<02:04, 83.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  15%|█▍        | 1.68G/11.3G [00:24<02:05, 82.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  15%|█▍        | 1.69G/11.3G [00:24<02:05, 82.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  15%|█▌        | 1.70G/11.3G [00:24<02:08, 79.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  15%|█▌        | 1.70G/11.3G [00:24<02:08, 80.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  15%|█▌        | 1.71G/11.3G [00:24<02:10, 78.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  15%|█▌        | 1.72G/11.3G [00:25<02:14, 76.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  15%|█▌        | 1.72G/11.3G [00:25<02:17, 74.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  15%|█▌        | 1.73G/11.3G [00:25<02:22, 71.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  15%|█▌        | 1.74G/11.3G [00:25<02:32, 67.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  15%|█▌        | 1.74G/11.3G [00:25<02:33, 66.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  16%|█▌        | 1.75G/11.3G [00:25<02:27, 69.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  16%|█▌        | 1.76G/11.3G [00:25<02:18, 73.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  16%|█▌        | 1.77G/11.3G [00:25<02:12, 76.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  16%|█▌        | 1.77G/11.3G [00:25<02:12, 77.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  16%|█▌        | 1.78G/11.3G [00:26<02:31, 67.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  16%|█▌        | 1.79G/11.3G [00:26<02:22, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  16%|█▌        | 1.80G/11.3G [00:26<02:21, 71.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  16%|█▌        | 1.80G/11.3G [00:26<02:20, 72.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  16%|█▌        | 1.81G/11.3G [00:26<02:16, 74.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  16%|█▌        | 1.82G/11.3G [00:26<02:18, 73.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  16%|█▌        | 1.82G/11.3G [00:26<02:29, 68.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  16%|█▌        | 1.83G/11.3G [00:26<02:35, 65.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  16%|█▋        | 1.84G/11.3G [00:26<02:30, 67.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  16%|█▋        | 1.85G/11.3G [00:27<02:22, 71.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  16%|█▋        | 1.85G/11.3G [00:27<02:27, 68.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  16%|█▋        | 1.86G/11.3G [00:27<02:21, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  17%|█▋        | 1.87G/11.3G [00:27<02:19, 72.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  17%|█▋        | 1.87G/11.3G [00:27<02:13, 75.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  17%|█▋        | 1.88G/11.3G [00:27<02:09, 78.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  17%|█▋        | 1.89G/11.3G [00:27<02:04, 80.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  17%|█▋        | 1.90G/11.3G [00:27<02:05, 79.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  17%|█▋        | 1.91G/11.3G [00:27<02:04, 80.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  17%|█▋        | 1.91G/11.3G [00:27<02:11, 76.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  17%|█▋        | 1.92G/11.3G [00:28<02:11, 76.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  17%|█▋        | 1.93G/11.3G [00:28<02:16, 73.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  17%|█▋        | 1.93G/11.3G [00:28<02:14, 74.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  17%|█▋        | 1.94G/11.3G [00:28<02:08, 77.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  17%|█▋        | 1.95G/11.3G [00:28<02:04, 80.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  17%|█▋        | 1.96G/11.3G [00:28<02:04, 80.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  17%|█▋        | 1.97G/11.3G [00:28<02:01, 82.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  18%|█▊        | 1.97G/11.3G [00:28<02:04, 80.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  18%|█▊        | 1.98G/11.3G [00:28<02:01, 82.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  18%|█▊        | 1.99G/11.3G [00:29<03:11, 52.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  18%|█▊        | 2.00G/11.3G [00:29<02:44, 60.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  18%|█▊        | 2.01G/11.3G [00:29<02:28, 67.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  18%|█▊        | 2.01G/11.3G [00:29<02:16, 72.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  18%|█▊        | 2.02G/11.3G [00:29<02:08, 77.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  18%|█▊        | 2.03G/11.3G [00:29<02:03, 80.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  18%|█▊        | 2.04G/11.3G [00:29<02:01, 81.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  18%|█▊        | 2.05G/11.3G [00:29<01:59, 82.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  18%|█▊        | 2.06G/11.3G [00:30<02:13, 74.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  18%|█▊        | 2.06G/11.3G [00:30<02:15, 73.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  18%|█▊        | 2.07G/11.3G [00:30<02:13, 73.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  18%|█▊        | 2.08G/11.3G [00:30<02:14, 73.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  18%|█▊        | 2.08G/11.3G [00:30<02:11, 75.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  19%|█▊        | 2.09G/11.3G [00:30<02:06, 77.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  19%|█▊        | 2.10G/11.3G [00:30<02:06, 78.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  19%|█▊        | 2.11G/11.3G [00:30<02:03, 80.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  19%|█▉        | 2.12G/11.3G [00:30<02:00, 81.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  19%|█▉        | 2.12G/11.3G [00:30<01:59, 82.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  19%|█▉        | 2.13G/11.3G [00:31<02:13, 73.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  19%|█▉        | 2.14G/11.3G [00:31<02:36, 62.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  19%|█▉        | 2.14G/11.3G [00:31<02:49, 57.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  19%|█▉        | 2.15G/11.3G [00:31<02:37, 62.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  19%|█▉        | 2.16G/11.3G [00:31<02:26, 67.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  19%|█▉        | 2.17G/11.3G [00:31<02:16, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  19%|█▉        | 2.17G/11.3G [00:31<02:12, 73.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  19%|█▉        | 2.18G/11.3G [00:31<02:15, 71.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  19%|█▉        | 2.19G/11.3G [00:32<02:13, 73.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  19%|█▉        | 2.20G/11.3G [00:32<02:07, 76.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  20%|█▉        | 2.20G/11.3G [00:32<02:06, 77.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  20%|█▉        | 2.21G/11.3G [00:32<02:02, 79.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  20%|█▉        | 2.22G/11.3G [00:32<02:00, 80.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  20%|█▉        | 2.23G/11.3G [00:32<02:04, 78.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  20%|█▉        | 2.23G/11.3G [00:32<02:07, 76.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  20%|█▉        | 2.24G/11.3G [00:32<02:08, 75.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  20%|█▉        | 2.25G/11.3G [00:32<02:06, 76.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  20%|██        | 2.26G/11.3G [00:32<02:08, 75.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  20%|██        | 2.26G/11.3G [00:33<02:23, 67.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  20%|██        | 2.27G/11.3G [00:33<02:19, 69.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  20%|██        | 2.28G/11.3G [00:33<02:11, 73.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  20%|██        | 2.28G/11.3G [00:33<02:10, 74.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  20%|██        | 2.29G/11.3G [00:33<02:09, 74.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  20%|██        | 2.30G/11.3G [00:33<02:05, 76.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  20%|██        | 2.31G/11.3G [00:33<02:06, 76.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  21%|██        | 2.31G/11.3G [00:33<02:07, 75.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  21%|██        | 2.32G/11.3G [00:33<02:15, 70.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  21%|██        | 2.33G/11.3G [00:34<02:13, 72.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  21%|██        | 2.33G/11.3G [00:34<02:25, 66.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  21%|██        | 2.34G/11.3G [00:34<02:24, 66.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  21%|██        | 2.35G/11.3G [00:34<02:20, 68.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  21%|██        | 2.35G/11.3G [00:34<02:29, 64.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  21%|██        | 2.36G/11.3G [00:34<02:26, 65.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  21%|██        | 2.37G/11.3G [00:34<03:22, 47.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  21%|██        | 2.37G/11.3G [00:34<02:49, 56.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  21%|██        | 2.38G/11.3G [00:35<02:28, 64.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  21%|██        | 2.39G/11.3G [00:35<02:16, 69.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  21%|██▏       | 2.40G/11.3G [00:35<02:07, 74.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  21%|██▏       | 2.41G/11.3G [00:35<02:01, 78.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  21%|██▏       | 2.41G/11.3G [00:35<02:01, 78.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  21%|██▏       | 2.42G/11.3G [00:35<02:03, 76.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  22%|██▏       | 2.43G/11.3G [00:35<02:06, 75.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  22%|██▏       | 2.44G/11.3G [00:35<02:02, 77.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  22%|██▏       | 2.44G/11.3G [00:35<02:09, 73.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  22%|██▏       | 2.45G/11.3G [00:35<02:10, 72.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  22%|██▏       | 2.46G/11.3G [00:36<02:08, 73.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  22%|██▏       | 2.47G/11.3G [00:36<02:08, 73.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  22%|██▏       | 2.47G/11.3G [00:36<02:10, 72.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  22%|██▏       | 2.48G/11.3G [00:36<02:04, 75.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  22%|██▏       | 2.49G/11.3G [00:36<02:02, 76.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  22%|██▏       | 2.50G/11.3G [00:36<02:02, 76.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  22%|██▏       | 2.50G/11.3G [00:36<01:58, 79.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  22%|██▏       | 2.51G/11.3G [00:36<02:06, 74.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  22%|██▏       | 2.52G/11.3G [00:36<02:05, 74.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  22%|██▏       | 2.52G/11.3G [00:37<02:27, 63.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  22%|██▏       | 2.53G/11.3G [00:37<02:23, 65.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  23%|██▎       | 2.54G/11.3G [00:37<02:17, 68.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  23%|██▎       | 2.54G/11.3G [00:37<02:17, 68.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  23%|██▎       | 2.55G/11.3G [00:37<02:08, 72.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  23%|██▎       | 2.56G/11.3G [00:37<02:02, 76.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  23%|██▎       | 2.57G/11.3G [00:37<02:21, 66.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  23%|██▎       | 2.57G/11.3G [00:37<02:21, 66.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  23%|██▎       | 2.58G/11.3G [00:37<02:14, 69.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  23%|██▎       | 2.59G/11.3G [00:38<02:12, 70.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  23%|██▎       | 2.60G/11.3G [00:38<02:04, 74.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  23%|██▎       | 2.60G/11.3G [00:38<02:03, 75.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  23%|██▎       | 2.61G/11.3G [00:38<01:59, 78.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  23%|██▎       | 2.62G/11.3G [00:38<01:56, 80.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  23%|██▎       | 2.63G/11.3G [00:38<02:06, 73.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  23%|██▎       | 2.63G/11.3G [00:38<02:03, 75.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  23%|██▎       | 2.64G/11.3G [00:38<01:58, 78.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  23%|██▎       | 2.65G/11.3G [00:38<01:58, 78.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  24%|██▎       | 2.66G/11.3G [00:38<01:54, 80.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  24%|██▎       | 2.67G/11.3G [00:39<01:54, 80.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  24%|██▎       | 2.67G/11.3G [00:39<02:03, 74.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  24%|██▍       | 2.68G/11.3G [00:39<02:05, 73.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  24%|██▍       | 2.69G/11.3G [00:39<02:04, 74.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  24%|██▍       | 2.69G/11.3G [00:39<01:59, 77.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  24%|██▍       | 2.70G/11.3G [00:39<01:55, 79.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  24%|██▍       | 2.71G/11.3G [00:39<01:59, 77.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  24%|██▍       | 2.72G/11.3G [00:39<01:56, 78.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  24%|██▍       | 2.73G/11.3G [00:39<01:54, 80.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  24%|██▍       | 2.73G/11.3G [00:40<01:51, 82.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  24%|██▍       | 2.74G/11.3G [00:40<01:59, 76.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  24%|██▍       | 2.75G/11.3G [00:40<01:55, 79.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  24%|██▍       | 2.76G/11.3G [00:40<02:00, 75.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  25%|██▍       | 2.76G/11.3G [00:40<02:00, 75.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  25%|██▍       | 2.77G/11.3G [00:40<02:39, 57.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  25%|██▍       | 2.78G/11.3G [00:40<02:23, 63.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  25%|██▍       | 2.79G/11.3G [00:40<02:14, 67.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  25%|██▍       | 2.79G/11.3G [00:40<02:08, 70.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  25%|██▍       | 2.80G/11.3G [00:41<02:14, 67.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  25%|██▍       | 2.81G/11.3G [00:41<02:08, 70.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  25%|██▍       | 2.81G/11.3G [00:41<02:13, 67.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  25%|██▌       | 2.82G/11.3G [00:41<02:05, 72.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  25%|██▌       | 2.83G/11.3G [00:41<02:10, 69.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  25%|██▌       | 2.84G/11.3G [00:41<02:10, 69.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  25%|██▌       | 2.84G/11.3G [00:41<02:10, 69.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  25%|██▌       | 2.85G/11.3G [00:41<02:02, 74.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  25%|██▌       | 2.86G/11.3G [00:41<02:02, 73.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  25%|██▌       | 2.87G/11.3G [00:42<01:59, 75.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  25%|██▌       | 2.87G/11.3G [00:42<02:01, 74.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  26%|██▌       | 2.88G/11.3G [00:42<01:57, 76.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  26%|██▌       | 2.89G/11.3G [00:42<01:58, 75.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  26%|██▌       | 2.89G/11.3G [00:42<02:01, 73.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  26%|██▌       | 2.90G/11.3G [00:42<01:58, 75.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  26%|██▌       | 2.91G/11.3G [00:42<01:56, 77.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  26%|██▌       | 2.92G/11.3G [00:42<01:52, 79.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  26%|██▌       | 2.92G/11.3G [00:42<01:53, 79.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  26%|██▌       | 2.93G/11.3G [00:42<01:53, 79.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  26%|██▌       | 2.94G/11.3G [00:43<02:01, 73.9MB/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  26%|██▌       | 2.95G/11.3G [00:43<01:56, 76.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  26%|██▌       | 2.96G/11.3G [00:43<01:52, 79.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  26%|██▋       | 2.96G/11.3G [00:43<01:50, 80.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  26%|██▋       | 2.97G/11.3G [00:43<01:49, 81.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  26%|██▋       | 2.98G/11.3G [00:43<01:51, 80.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  26%|██▋       | 2.99G/11.3G [00:43<01:49, 81.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  27%|██▋       | 2.99G/11.3G [00:43<01:47, 82.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  27%|██▋       | 3.00G/11.3G [00:43<01:50, 80.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  27%|██▋       | 3.01G/11.3G [00:43<01:50, 80.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  27%|██▋       | 3.02G/11.3G [00:44<01:48, 81.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  27%|██▋       | 3.03G/11.3G [00:44<01:47, 82.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  27%|██▋       | 3.03G/11.3G [00:44<01:46, 82.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  27%|██▋       | 3.04G/11.3G [00:44<01:48, 81.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  27%|██▋       | 3.05G/11.3G [00:44<01:49, 80.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  27%|██▋       | 3.06G/11.3G [00:44<01:53, 78.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  27%|██▋       | 3.06G/11.3G [00:44<01:49, 80.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  27%|██▋       | 3.07G/11.3G [00:44<01:51, 79.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  27%|██▋       | 3.08G/11.3G [00:44<01:49, 80.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  27%|██▋       | 3.09G/11.3G [00:45<02:06, 69.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  27%|██▋       | 3.09G/11.3G [00:45<02:04, 70.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  28%|██▊       | 3.10G/11.3G [00:45<01:59, 73.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  28%|██▊       | 3.11G/11.3G [00:45<02:00, 72.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  28%|██▊       | 3.11G/11.3G [00:45<02:02, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  28%|██▊       | 3.12G/11.3G [00:45<01:57, 74.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  28%|██▊       | 3.13G/11.3G [00:45<01:53, 76.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  28%|██▊       | 3.14G/11.3G [00:45<01:50, 79.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  28%|██▊       | 3.15G/11.3G [00:45<01:53, 76.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  28%|██▊       | 3.15G/11.3G [00:45<01:50, 78.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  28%|██▊       | 3.16G/11.3G [00:46<01:48, 80.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  28%|██▊       | 3.17G/11.3G [00:46<01:47, 80.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  28%|██▊       | 3.18G/11.3G [00:46<01:47, 81.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  28%|██▊       | 3.18G/11.3G [00:46<01:47, 81.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  28%|██▊       | 3.19G/11.3G [00:46<01:50, 78.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  28%|██▊       | 3.20G/11.3G [00:46<01:47, 80.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  28%|██▊       | 3.21G/11.3G [00:46<01:49, 79.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  29%|██▊       | 3.21G/11.3G [00:46<01:47, 80.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  29%|██▊       | 3.22G/11.3G [00:46<01:47, 80.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  29%|██▊       | 3.23G/11.3G [00:47<01:45, 81.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  29%|██▊       | 3.24G/11.3G [00:47<01:44, 82.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  29%|██▉       | 3.25G/11.3G [00:47<01:45, 82.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  29%|██▉       | 3.25G/11.3G [00:47<01:49, 78.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  29%|██▉       | 3.26G/11.3G [00:47<01:47, 80.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  29%|██▉       | 3.27G/11.3G [00:47<01:45, 81.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  29%|██▉       | 3.28G/11.3G [00:47<01:48, 79.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  29%|██▉       | 3.28G/11.3G [00:47<01:53, 75.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  29%|██▉       | 3.29G/11.3G [00:47<01:55, 74.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  29%|██▉       | 3.30G/11.3G [00:47<01:55, 74.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  29%|██▉       | 3.31G/11.3G [00:48<01:52, 75.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  29%|██▉       | 3.31G/11.3G [00:48<01:49, 78.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  29%|██▉       | 3.32G/11.3G [00:48<01:47, 79.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  30%|██▉       | 3.33G/11.3G [00:48<01:48, 78.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  30%|██▉       | 3.34G/11.3G [00:48<01:46, 80.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  30%|██▉       | 3.34G/11.3G [00:48<01:45, 80.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  30%|██▉       | 3.35G/11.3G [00:48<01:43, 82.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  30%|██▉       | 3.36G/11.3G [00:48<01:42, 83.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  30%|██▉       | 3.37G/11.3G [00:48<02:00, 70.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  30%|██▉       | 3.38G/11.3G [00:49<02:09, 65.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  30%|██▉       | 3.38G/11.3G [00:49<02:08, 65.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  30%|███       | 3.39G/11.3G [00:49<02:01, 69.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  30%|███       | 3.40G/11.3G [00:49<01:54, 73.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  30%|███       | 3.41G/11.3G [00:49<01:51, 75.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  30%|███       | 3.41G/11.3G [00:49<01:49, 77.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  30%|███       | 3.42G/11.3G [00:49<01:46, 78.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  30%|███       | 3.43G/11.3G [00:49<01:44, 80.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  30%|███       | 3.44G/11.3G [00:49<01:45, 79.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  31%|███       | 3.44G/11.3G [00:50<01:49, 76.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  31%|███       | 3.45G/11.3G [00:50<01:47, 78.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  31%|███       | 3.46G/11.3G [00:50<01:45, 79.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  31%|███       | 3.47G/11.3G [00:50<01:48, 77.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  31%|███       | 3.47G/11.3G [00:50<01:46, 78.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  31%|███       | 3.48G/11.3G [00:50<01:49, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  31%|███       | 3.49G/11.3G [00:50<01:53, 73.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  31%|███       | 3.50G/11.3G [00:50<02:55, 47.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  31%|███       | 3.50G/11.3G [00:51<02:28, 56.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  31%|███       | 3.51G/11.3G [00:51<02:12, 62.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  31%|███       | 3.52G/11.3G [00:51<02:00, 69.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  31%|███▏      | 3.53G/11.3G [00:51<01:50, 75.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  31%|███▏      | 3.54G/11.3G [00:51<01:42, 81.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  31%|███▏      | 3.55G/11.3G [00:51<01:38, 84.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  32%|███▏      | 3.55G/11.3G [00:51<01:36, 86.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  32%|███▏      | 3.56G/11.3G [00:51<01:58, 69.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  32%|███▏      | 3.57G/11.3G [00:51<01:53, 72.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  32%|███▏      | 3.58G/11.3G [00:52<01:50, 74.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  32%|███▏      | 3.59G/11.3G [00:52<01:47, 76.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  32%|███▏      | 3.59G/11.3G [00:52<01:46, 77.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  32%|███▏      | 3.60G/11.3G [00:52<01:51, 73.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  32%|███▏      | 3.61G/11.3G [00:52<01:53, 72.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  32%|███▏      | 3.61G/11.3G [00:52<01:57, 70.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  32%|███▏      | 3.62G/11.3G [00:52<01:53, 72.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  32%|███▏      | 3.63G/11.3G [00:52<01:47, 76.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  32%|███▏      | 3.64G/11.3G [00:52<01:51, 73.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  32%|███▏      | 3.64G/11.3G [00:52<01:52, 72.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  32%|███▏      | 3.65G/11.3G [00:53<02:01, 67.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  32%|███▏      | 3.66G/11.3G [00:53<01:55, 70.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  33%|███▎      | 3.67G/11.3G [00:53<01:52, 72.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  33%|███▎      | 3.67G/11.3G [00:53<01:54, 71.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  33%|███▎      | 3.68G/11.3G [00:53<01:49, 74.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  33%|███▎      | 3.69G/11.3G [00:53<01:42, 79.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  33%|███▎      | 3.70G/11.3G [00:53<01:39, 81.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  33%|███▎      | 3.71G/11.3G [00:53<01:36, 83.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  33%|███▎      | 3.71G/11.3G [00:53<01:34, 86.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  33%|███▎      | 3.72G/11.3G [00:54<01:35, 85.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  33%|███▎      | 3.73G/11.3G [00:54<01:34, 85.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  33%|███▎      | 3.74G/11.3G [00:54<01:34, 85.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  33%|███▎      | 3.75G/11.3G [00:54<01:38, 82.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  33%|███▎      | 3.75G/11.3G [00:54<01:43, 78.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  33%|███▎      | 3.76G/11.3G [00:54<01:39, 81.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  33%|███▎      | 3.77G/11.3G [00:54<01:53, 71.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  34%|███▎      | 3.78G/11.3G [00:54<01:47, 74.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  34%|███▎      | 3.78G/11.3G [00:54<01:45, 76.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  34%|███▎      | 3.79G/11.3G [00:55<01:43, 77.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  34%|███▎      | 3.80G/11.3G [00:55<01:41, 78.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  34%|███▍      | 3.81G/11.3G [00:55<01:41, 78.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  34%|███▍      | 3.81G/11.3G [00:55<01:41, 79.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  34%|███▍      | 3.82G/11.3G [00:55<01:38, 81.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  34%|███▍      | 3.83G/11.3G [00:55<01:36, 82.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  34%|███▍      | 3.84G/11.3G [00:55<01:37, 81.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  34%|███▍      | 3.85G/11.3G [00:55<01:38, 81.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  34%|███▍      | 3.85G/11.3G [00:55<01:39, 79.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  34%|███▍      | 3.86G/11.3G [00:55<01:37, 81.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  34%|███▍      | 3.87G/11.3G [00:56<01:48, 73.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  34%|███▍      | 3.88G/11.3G [00:56<01:48, 73.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  34%|███▍      | 3.88G/11.3G [00:56<02:09, 61.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  35%|███▍      | 3.89G/11.3G [00:56<01:58, 67.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  35%|███▍      | 3.90G/11.3G [00:56<01:54, 69.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  35%|███▍      | 3.91G/11.3G [00:56<01:48, 72.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  35%|███▍      | 3.91G/11.3G [00:56<01:42, 76.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  35%|███▍      | 3.92G/11.3G [00:56<01:42, 77.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  35%|███▍      | 3.93G/11.3G [00:56<01:44, 75.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  35%|███▍      | 3.94G/11.3G [00:57<01:46, 74.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  35%|███▍      | 3.94G/11.3G [00:57<01:47, 73.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  35%|███▌      | 3.95G/11.3G [00:57<01:49, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  35%|███▌      | 3.96G/11.3G [00:57<01:50, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  35%|███▌      | 3.96G/11.3G [00:57<01:47, 72.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  35%|███▌      | 3.97G/11.3G [00:57<01:47, 72.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  35%|███▌      | 3.98G/11.3G [00:57<01:43, 75.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  35%|███▌      | 3.98G/11.3G [00:57<02:00, 65.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  35%|███▌      | 3.99G/11.3G [00:57<01:47, 72.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  36%|███▌      | 4.00G/11.3G [00:58<01:39, 78.8MB/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  36%|███▌      | 4.01G/11.3G [00:58<01:33, 83.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  36%|███▌      | 4.02G/11.3G [00:58<02:14, 57.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  36%|███▌      | 4.03G/11.3G [00:58<02:23, 54.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  36%|███▌      | 4.03G/11.3G [00:58<02:11, 59.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  36%|███▌      | 4.04G/11.3G [00:58<02:01, 64.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  36%|███▌      | 4.05G/11.3G [00:58<01:53, 68.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  36%|███▌      | 4.06G/11.3G [00:58<01:47, 71.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  36%|███▌      | 4.06G/11.3G [00:59<01:43, 74.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  36%|███▌      | 4.07G/11.3G [00:59<01:56, 66.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  36%|███▌      | 4.08G/11.3G [00:59<01:59, 64.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  36%|███▌      | 4.08G/11.3G [00:59<01:50, 70.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  36%|███▋      | 4.09G/11.3G [00:59<01:48, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  36%|███▋      | 4.10G/11.3G [00:59<01:45, 73.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  36%|███▋      | 4.11G/11.3G [00:59<01:42, 75.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  36%|███▋      | 4.11G/11.3G [00:59<01:47, 71.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  37%|███▋      | 4.12G/11.3G [00:59<01:40, 76.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  37%|███▋      | 4.13G/11.3G [01:00<01:36, 79.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  37%|███▋      | 4.14G/11.3G [01:00<02:03, 62.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  37%|███▋      | 4.14G/11.3G [01:00<01:53, 67.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  37%|███▋      | 4.15G/11.3G [01:00<01:43, 73.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  37%|███▋      | 4.16G/11.3G [01:00<01:37, 78.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  37%|███▋      | 4.17G/11.3G [01:00<01:32, 82.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  37%|███▋      | 4.18G/11.3G [01:00<01:29, 84.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  37%|███▋      | 4.19G/11.3G [01:00<01:27, 87.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  37%|███▋      | 4.20G/11.3G [01:00<01:32, 81.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  37%|███▋      | 4.20G/11.3G [01:01<01:49, 69.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  37%|███▋      | 4.21G/11.3G [01:01<01:44, 72.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  37%|███▋      | 4.22G/11.3G [01:01<01:51, 68.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  37%|███▋      | 4.23G/11.3G [01:01<02:01, 62.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  38%|███▊      | 4.23G/11.3G [01:01<01:53, 66.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  38%|███▊      | 4.24G/11.3G [01:01<01:51, 67.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  38%|███▊      | 4.25G/11.3G [01:01<02:00, 62.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  38%|███▊      | 4.25G/11.3G [01:01<01:54, 66.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  38%|███▊      | 4.26G/11.3G [01:02<01:50, 68.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  38%|███▊      | 4.27G/11.3G [01:02<01:55, 65.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  38%|███▊      | 4.27G/11.3G [01:02<01:54, 65.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  38%|███▊      | 4.28G/11.3G [01:02<01:51, 67.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  38%|███▊      | 4.29G/11.3G [01:02<02:06, 59.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  38%|███▊      | 4.29G/11.3G [01:02<01:54, 65.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  38%|███▊      | 4.30G/11.3G [01:02<01:46, 70.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  38%|███▊      | 4.31G/11.3G [01:02<01:45, 70.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  38%|███▊      | 4.31G/11.3G [01:06<18:45, 6.64MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  38%|███▊      | 4.32G/11.3G [01:06<14:02, 8.87MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  38%|███▊      | 4.33G/11.3G [01:06<10:29, 11.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  38%|███▊      | 4.33G/11.3G [01:06<07:22, 16.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  39%|███▊      | 4.34G/11.3G [01:06<05:22, 23.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  39%|███▊      | 4.35G/11.3G [01:06<04:05, 30.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  39%|███▊      | 4.36G/11.3G [01:06<03:14, 38.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  39%|███▊      | 4.37G/11.3G [01:06<02:40, 46.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  39%|███▉      | 4.38G/11.3G [01:07<02:16, 54.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  39%|███▉      | 4.38G/11.3G [01:07<02:00, 61.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  39%|███▉      | 4.39G/11.3G [01:07<01:48, 68.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  39%|███▉      | 4.40G/11.3G [01:07<01:39, 74.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  39%|███▉      | 4.41G/11.3G [01:07<01:33, 78.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  39%|███▉      | 4.42G/11.3G [01:07<01:29, 82.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  39%|███▉      | 4.43G/11.3G [01:07<02:14, 54.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  39%|███▉      | 4.43G/11.3G [01:07<01:58, 61.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  39%|███▉      | 4.44G/11.3G [01:08<01:48, 67.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  39%|███▉      | 4.45G/11.3G [01:08<01:40, 73.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  40%|███▉      | 4.46G/11.3G [01:08<01:34, 77.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  40%|███▉      | 4.47G/11.3G [01:08<01:31, 80.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  40%|███▉      | 4.48G/11.3G [01:08<01:28, 82.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  40%|███▉      | 4.48G/11.3G [01:08<01:26, 84.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  40%|███▉      | 4.49G/11.3G [01:08<01:25, 85.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  40%|███▉      | 4.50G/11.3G [01:08<01:24, 86.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  40%|███▉      | 4.51G/11.3G [01:08<01:23, 87.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  40%|████      | 4.52G/11.3G [01:08<01:22, 87.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  40%|████      | 4.53G/11.3G [01:09<01:22, 88.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  40%|████      | 4.53G/11.3G [01:09<01:30, 79.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  40%|████      | 4.54G/11.3G [01:09<01:29, 80.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  40%|████      | 4.55G/11.3G [01:09<01:29, 81.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  40%|████      | 4.56G/11.3G [01:09<01:28, 81.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  40%|████      | 4.57G/11.3G [01:09<01:27, 82.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  41%|████      | 4.57G/11.3G [01:09<01:25, 83.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  41%|████      | 4.58G/11.3G [01:09<01:24, 84.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  41%|████      | 4.59G/11.3G [01:09<01:25, 84.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  41%|████      | 4.60G/11.3G [01:10<01:27, 82.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  41%|████      | 4.60G/11.3G [01:10<01:29, 79.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  41%|████      | 4.61G/11.3G [01:10<01:28, 80.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  41%|████      | 4.62G/11.3G [01:10<01:27, 81.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  41%|████      | 4.63G/11.3G [01:10<01:26, 82.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  41%|████      | 4.64G/11.3G [01:10<01:26, 82.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  41%|████      | 4.64G/11.3G [01:10<01:26, 82.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  41%|████▏     | 4.65G/11.3G [01:10<01:33, 75.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  41%|████▏     | 4.66G/11.3G [01:10<01:43, 68.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  41%|████▏     | 4.67G/11.3G [01:10<01:40, 70.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  41%|████▏     | 4.67G/11.3G [01:11<01:38, 72.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  42%|████▏     | 4.68G/11.3G [01:11<01:34, 74.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  42%|████▏     | 4.69G/11.3G [01:11<01:37, 72.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  42%|████▏     | 4.69G/11.3G [01:11<01:38, 72.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  42%|████▏     | 4.70G/11.3G [01:11<01:50, 63.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  42%|████▏     | 4.71G/11.3G [01:11<01:44, 67.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  42%|████▏     | 4.72G/11.3G [01:11<01:37, 72.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  42%|████▏     | 4.72G/11.3G [01:11<01:32, 75.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  42%|████▏     | 4.73G/11.3G [01:11<01:29, 78.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  42%|████▏     | 4.74G/11.3G [01:12<01:26, 80.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  42%|████▏     | 4.75G/11.3G [01:12<01:25, 82.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  42%|████▏     | 4.76G/11.3G [01:12<01:23, 83.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  42%|████▏     | 4.76G/11.3G [01:12<01:37, 71.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  42%|████▏     | 4.77G/11.3G [01:12<01:32, 75.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  42%|████▏     | 4.78G/11.3G [01:12<01:28, 78.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  42%|████▏     | 4.79G/11.3G [01:12<01:27, 79.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  43%|████▎     | 4.80G/11.3G [01:12<01:28, 78.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  43%|████▎     | 4.80G/11.3G [01:12<01:26, 80.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  43%|████▎     | 4.81G/11.3G [01:13<01:28, 78.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  43%|████▎     | 4.82G/11.3G [01:13<01:30, 77.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  43%|████▎     | 4.83G/11.3G [01:13<01:36, 72.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  43%|████▎     | 4.83G/11.3G [01:13<01:31, 75.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  43%|████▎     | 4.84G/11.3G [01:13<01:34, 73.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  43%|████▎     | 4.85G/11.3G [01:13<01:28, 78.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  43%|████▎     | 4.86G/11.3G [01:13<01:26, 79.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  43%|████▎     | 4.86G/11.3G [01:13<01:25, 80.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  43%|████▎     | 4.87G/11.3G [01:13<01:24, 81.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  43%|████▎     | 4.88G/11.3G [01:13<01:25, 80.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  43%|████▎     | 4.89G/11.3G [01:14<01:24, 80.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  43%|████▎     | 4.89G/11.3G [01:14<01:24, 81.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  43%|████▎     | 4.90G/11.3G [01:14<01:27, 78.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  44%|████▎     | 4.91G/11.3G [01:14<01:29, 76.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  44%|████▎     | 4.92G/11.3G [01:14<01:27, 77.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  44%|████▎     | 4.92G/11.3G [01:14<01:29, 76.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  44%|████▍     | 4.93G/11.3G [01:14<01:25, 79.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  44%|████▍     | 4.94G/11.3G [01:14<01:23, 81.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  44%|████▍     | 4.95G/11.3G [01:14<01:22, 82.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  44%|████▍     | 4.96G/11.3G [01:14<01:22, 82.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  44%|████▍     | 4.96G/11.3G [01:15<01:20, 84.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  44%|████▍     | 4.97G/11.3G [01:15<01:19, 84.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  44%|████▍     | 4.98G/11.3G [01:15<01:22, 82.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  44%|████▍     | 4.99G/11.3G [01:15<01:22, 82.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  44%|████▍     | 5.00G/11.3G [01:15<01:22, 81.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  44%|████▍     | 5.00G/11.3G [01:15<01:20, 84.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  44%|████▍     | 5.01G/11.3G [01:15<01:26, 77.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  45%|████▍     | 5.02G/11.3G [01:15<01:23, 80.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  45%|████▍     | 5.03G/11.3G [01:15<01:21, 81.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  45%|████▍     | 5.04G/11.3G [01:16<01:25, 78.0MB/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  45%|████▍     | 5.04G/11.3G [01:16<01:23, 79.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  45%|████▍     | 5.05G/11.3G [01:16<01:30, 73.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  45%|████▍     | 5.06G/11.3G [01:16<01:27, 76.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  45%|████▍     | 5.07G/11.3G [01:16<01:24, 78.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  45%|████▌     | 5.08G/11.3G [01:16<01:21, 81.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  45%|████▌     | 5.08G/11.3G [01:16<01:27, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  45%|████▌     | 5.09G/11.3G [01:16<01:25, 77.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  45%|████▌     | 5.10G/11.3G [01:16<01:32, 71.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  45%|████▌     | 5.10G/11.3G [01:17<01:31, 72.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  45%|████▌     | 5.11G/11.3G [01:17<01:27, 75.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  45%|████▌     | 5.12G/11.3G [01:17<01:31, 72.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  45%|████▌     | 5.13G/11.3G [01:17<01:28, 74.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  46%|████▌     | 5.13G/11.3G [01:17<01:30, 73.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  46%|████▌     | 5.14G/11.3G [01:17<01:26, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  46%|████▌     | 5.15G/11.3G [01:17<01:22, 79.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  46%|████▌     | 5.16G/11.3G [01:17<01:22, 79.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  46%|████▌     | 5.17G/11.3G [01:17<01:20, 81.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  46%|████▌     | 5.17G/11.3G [01:17<01:28, 74.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  46%|████▌     | 5.18G/11.3G [01:18<01:27, 74.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  46%|████▌     | 5.19G/11.3G [01:18<01:25, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  46%|████▌     | 5.20G/11.3G [01:18<01:23, 77.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  46%|████▌     | 5.20G/11.3G [01:18<01:37, 67.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  46%|████▌     | 5.21G/11.3G [01:18<01:30, 71.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  46%|████▋     | 5.22G/11.3G [01:18<01:25, 76.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  46%|████▋     | 5.23G/11.3G [01:18<01:32, 70.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  46%|████▋     | 5.23G/11.3G [01:18<01:27, 74.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  46%|████▋     | 5.24G/11.3G [01:18<01:23, 77.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  47%|████▋     | 5.25G/11.3G [01:19<01:19, 81.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  47%|████▋     | 5.26G/11.3G [01:19<01:18, 82.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  47%|████▋     | 5.27G/11.3G [01:19<01:20, 80.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  47%|████▋     | 5.27G/11.3G [01:19<01:29, 71.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  47%|████▋     | 5.28G/11.3G [01:19<01:25, 75.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  47%|████▋     | 5.29G/11.3G [01:19<01:22, 78.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  47%|████▋     | 5.30G/11.3G [01:19<01:21, 78.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  47%|████▋     | 5.30G/11.3G [01:19<01:20, 79.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  47%|████▋     | 5.31G/11.3G [01:19<01:19, 80.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  47%|████▋     | 5.32G/11.3G [01:20<01:17, 82.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  47%|████▋     | 5.33G/11.3G [01:20<01:15, 84.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  47%|████▋     | 5.34G/11.3G [01:20<01:15, 84.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  47%|████▋     | 5.34G/11.3G [01:20<01:19, 80.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  47%|████▋     | 5.35G/11.3G [01:20<01:19, 80.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  48%|████▊     | 5.36G/11.3G [01:20<01:17, 82.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  48%|████▊     | 5.37G/11.3G [01:20<01:18, 80.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  48%|████▊     | 5.38G/11.3G [01:20<01:16, 82.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  48%|████▊     | 5.38G/11.3G [01:20<01:17, 81.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  48%|████▊     | 5.39G/11.3G [01:20<01:22, 76.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  48%|████▊     | 5.40G/11.3G [01:21<01:28, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  48%|████▊     | 5.41G/11.3G [01:21<01:22, 76.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  48%|████▊     | 5.41G/11.3G [01:21<01:23, 75.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  48%|████▊     | 5.42G/11.3G [01:21<01:20, 78.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  48%|████▊     | 5.43G/11.3G [01:21<01:19, 79.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  48%|████▊     | 5.44G/11.3G [01:21<01:24, 74.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  48%|████▊     | 5.44G/11.3G [01:21<01:20, 77.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  48%|████▊     | 5.45G/11.3G [01:21<01:19, 78.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  48%|████▊     | 5.46G/11.3G [01:21<01:20, 77.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  48%|████▊     | 5.47G/11.3G [01:22<01:20, 77.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  49%|████▊     | 5.47G/11.3G [01:22<01:17, 80.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  49%|████▊     | 5.48G/11.3G [01:22<01:29, 69.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  49%|████▊     | 5.49G/11.3G [01:22<01:22, 74.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  49%|████▉     | 5.50G/11.3G [01:22<01:20, 77.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  49%|████▉     | 5.51G/11.3G [01:22<01:24, 73.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  49%|████▉     | 5.51G/11.3G [01:22<01:21, 75.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  49%|████▉     | 5.52G/11.3G [01:22<01:18, 79.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  49%|████▉     | 5.53G/11.3G [01:22<01:20, 76.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  49%|████▉     | 5.54G/11.3G [01:23<01:19, 78.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  49%|████▉     | 5.54G/11.3G [01:23<01:28, 69.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  49%|████▉     | 5.55G/11.3G [01:23<01:25, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  49%|████▉     | 5.56G/11.3G [01:23<01:23, 73.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  49%|████▉     | 5.57G/11.3G [01:23<01:26, 71.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  49%|████▉     | 5.57G/11.3G [01:23<01:24, 72.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  49%|████▉     | 5.58G/11.3G [01:23<01:19, 77.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  50%|████▉     | 5.59G/11.3G [01:23<01:16, 79.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  50%|████▉     | 5.60G/11.3G [01:23<01:16, 79.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  50%|████▉     | 5.60G/11.3G [01:24<01:20, 75.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  50%|████▉     | 5.61G/11.3G [01:24<01:21, 74.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  50%|████▉     | 5.62G/11.3G [01:24<01:17, 78.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  50%|████▉     | 5.63G/11.3G [01:24<01:16, 79.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  50%|████▉     | 5.63G/11.3G [01:24<01:13, 81.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  50%|█████     | 5.64G/11.3G [01:24<01:12, 83.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  50%|█████     | 5.65G/11.3G [01:24<01:12, 83.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  50%|█████     | 5.66G/11.3G [01:24<01:10, 85.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  50%|█████     | 5.67G/11.3G [01:24<01:11, 84.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  50%|█████     | 5.68G/11.3G [01:24<01:11, 84.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  50%|█████     | 5.68G/11.3G [01:25<01:13, 81.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  50%|█████     | 5.69G/11.3G [01:25<01:18, 76.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  51%|█████     | 5.70G/11.3G [01:25<01:37, 61.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  51%|█████     | 5.70G/11.3G [01:25<01:34, 63.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  51%|█████     | 5.71G/11.3G [01:25<01:31, 65.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  51%|█████     | 5.72G/11.3G [01:25<02:09, 46.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  51%|█████     | 5.73G/11.3G [01:25<01:46, 56.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  51%|█████     | 5.73G/11.3G [01:26<01:31, 65.2MB/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  51%|█████     | 5.74G/11.3G [01:26<01:21, 72.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  51%|█████     | 5.75G/11.3G [01:26<01:15, 78.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  51%|█████     | 5.76G/11.3G [01:26<01:11, 83.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  51%|█████     | 5.77G/11.3G [01:26<01:09, 84.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  51%|█████     | 5.78G/11.3G [01:26<01:13, 80.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  51%|█████▏    | 5.79G/11.3G [01:26<01:12, 81.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  51%|█████▏    | 5.79G/11.3G [01:26<01:12, 80.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  51%|█████▏    | 5.80G/11.3G [01:26<01:11, 81.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  52%|█████▏    | 5.81G/11.3G [01:26<01:09, 84.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  52%|█████▏    | 5.82G/11.3G [01:27<01:12, 80.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  52%|█████▏    | 5.83G/11.3G [01:27<01:15, 77.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  52%|█████▏    | 5.83G/11.3G [01:27<01:19, 73.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  52%|█████▏    | 5.84G/11.3G [01:27<01:18, 73.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  52%|█████▏    | 5.85G/11.3G [01:27<01:22, 70.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  52%|█████▏    | 5.85G/11.3G [01:27<01:21, 71.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  52%|█████▏    | 5.86G/11.3G [01:27<01:16, 76.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  52%|█████▏    | 5.87G/11.3G [01:27<01:18, 73.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  52%|█████▏    | 5.88G/11.3G [01:27<01:15, 76.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  52%|█████▏    | 5.89G/11.3G [01:28<01:12, 79.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  52%|█████▏    | 5.89G/11.3G [01:28<01:11, 80.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  52%|█████▏    | 5.90G/11.3G [01:28<01:13, 78.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  52%|█████▏    | 5.91G/11.3G [01:28<01:10, 81.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  52%|█████▏    | 5.92G/11.3G [01:28<01:13, 78.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  53%|█████▎    | 5.92G/11.3G [01:28<01:10, 81.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  53%|█████▎    | 5.93G/11.3G [01:28<01:10, 81.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  53%|█████▎    | 5.94G/11.3G [01:28<01:09, 82.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  53%|█████▎    | 5.95G/11.3G [01:28<01:14, 77.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  53%|█████▎    | 5.96G/11.3G [01:28<01:13, 77.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  53%|█████▎    | 5.96G/11.3G [01:29<01:11, 80.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  53%|█████▎    | 5.97G/11.3G [01:29<01:11, 79.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  53%|█████▎    | 5.98G/11.3G [01:29<01:15, 75.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  53%|█████▎    | 5.99G/11.3G [01:29<01:16, 73.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  53%|█████▎    | 5.99G/11.3G [01:29<01:13, 77.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  53%|█████▎    | 6.00G/11.3G [01:29<01:10, 80.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  53%|█████▎    | 6.01G/11.3G [01:29<01:11, 78.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  53%|█████▎    | 6.02G/11.3G [01:29<01:10, 80.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  53%|█████▎    | 6.03G/11.3G [01:29<01:08, 82.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  54%|█████▎    | 6.03G/11.3G [01:30<01:07, 82.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  54%|█████▎    | 6.04G/11.3G [01:30<01:05, 85.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  54%|█████▎    | 6.05G/11.3G [01:30<01:07, 83.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  54%|█████▎    | 6.06G/11.3G [01:30<01:05, 85.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  54%|█████▍    | 6.07G/11.3G [01:30<01:07, 82.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  54%|█████▍    | 6.07G/11.3G [01:30<01:06, 83.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  54%|█████▍    | 6.08G/11.3G [01:30<01:05, 85.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  54%|█████▍    | 6.09G/11.3G [01:30<01:04, 86.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  54%|█████▍    | 6.10G/11.3G [01:30<01:03, 87.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  54%|█████▍    | 6.11G/11.3G [01:30<01:03, 86.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  54%|█████▍    | 6.12G/11.3G [01:31<01:03, 87.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  54%|█████▍    | 6.12G/11.3G [01:31<01:02, 88.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  54%|█████▍    | 6.13G/11.3G [01:31<01:05, 84.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  54%|█████▍    | 6.14G/11.3G [01:31<01:07, 82.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  55%|█████▍    | 6.15G/11.3G [01:31<01:10, 78.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  55%|█████▍    | 6.16G/11.3G [01:31<01:11, 76.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  55%|█████▍    | 6.16G/11.3G [01:31<01:09, 78.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  55%|█████▍    | 6.17G/11.3G [01:31<01:10, 77.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  55%|█████▍    | 6.18G/11.3G [01:31<01:10, 77.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  55%|█████▍    | 6.19G/11.3G [01:32<01:08, 79.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  55%|█████▍    | 6.19G/11.3G [01:32<01:06, 82.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  55%|█████▌    | 6.20G/11.3G [01:32<01:08, 79.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  55%|█████▌    | 6.21G/11.3G [01:32<01:06, 81.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  55%|█████▌    | 6.22G/11.3G [01:32<01:08, 79.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  55%|█████▌    | 6.23G/11.3G [01:32<01:07, 80.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  55%|█████▌    | 6.23G/11.3G [01:32<01:07, 80.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  55%|█████▌    | 6.24G/11.3G [01:32<01:08, 78.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  55%|█████▌    | 6.25G/11.3G [01:32<01:08, 79.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  55%|█████▌    | 6.26G/11.3G [01:32<01:08, 78.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  56%|█████▌    | 6.26G/11.3G [01:33<01:18, 68.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  56%|█████▌    | 6.27G/11.3G [01:33<01:15, 71.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  56%|█████▌    | 6.28G/11.3G [01:33<01:11, 75.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  56%|█████▌    | 6.29G/11.3G [01:33<01:07, 79.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  56%|█████▌    | 6.29G/11.3G [01:33<01:07, 79.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  56%|█████▌    | 6.30G/11.3G [01:33<01:05, 81.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  56%|█████▌    | 6.31G/11.3G [01:33<01:06, 80.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  56%|█████▌    | 6.32G/11.3G [01:33<01:06, 80.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  56%|█████▌    | 6.32G/11.3G [01:33<01:06, 79.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  56%|█████▌    | 6.33G/11.3G [01:34<01:06, 80.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  56%|█████▌    | 6.34G/11.3G [01:34<01:07, 78.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  56%|█████▋    | 6.35G/11.3G [01:34<01:05, 80.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  56%|█████▋    | 6.36G/11.3G [01:34<01:08, 76.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  56%|█████▋    | 6.36G/11.3G [01:34<01:07, 78.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  56%|█████▋    | 6.37G/11.3G [01:34<01:10, 74.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  57%|█████▋    | 6.38G/11.3G [01:34<01:12, 72.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  57%|█████▋    | 6.38G/11.3G [01:34<01:15, 69.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  57%|█████▋    | 6.39G/11.3G [01:34<01:16, 68.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  57%|█████▋    | 6.40G/11.3G [01:34<01:15, 69.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  57%|█████▋    | 6.40G/11.3G [01:35<01:11, 73.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  57%|█████▋    | 6.41G/11.3G [01:35<01:07, 76.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  57%|█████▋    | 6.42G/11.3G [01:35<01:06, 78.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  57%|█████▋    | 6.43G/11.3G [01:35<01:10, 73.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  57%|█████▋    | 6.43G/11.3G [01:35<01:10, 74.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  57%|█████▋    | 6.44G/11.3G [01:35<01:05, 78.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  57%|█████▋    | 6.45G/11.3G [01:35<01:03, 81.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  57%|█████▋    | 6.46G/11.3G [01:35<01:05, 79.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  57%|█████▋    | 6.47G/11.3G [01:35<01:05, 79.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  57%|█████▋    | 6.48G/11.3G [01:36<01:02, 82.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  57%|█████▋    | 6.48G/11.3G [01:36<01:02, 82.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  58%|█████▊    | 6.49G/11.3G [01:36<01:01, 83.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  58%|█████▊    | 6.50G/11.3G [01:36<01:00, 84.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  58%|█████▊    | 6.51G/11.3G [01:36<01:00, 84.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  58%|█████▊    | 6.51G/11.3G [01:36<01:00, 84.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  58%|█████▊    | 6.52G/11.3G [01:36<01:00, 85.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  58%|█████▊    | 6.53G/11.3G [01:36<01:03, 79.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  58%|█████▊    | 6.54G/11.3G [01:36<01:06, 76.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  58%|█████▊    | 6.55G/11.3G [01:36<01:04, 78.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  58%|█████▊    | 6.55G/11.3G [01:37<01:03, 79.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  58%|█████▊    | 6.56G/11.3G [01:37<01:46, 47.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  58%|█████▊    | 6.57G/11.3G [01:37<01:27, 57.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  58%|█████▊    | 6.58G/11.3G [01:37<01:15, 66.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  58%|█████▊    | 6.59G/11.3G [01:37<01:07, 74.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  59%|█████▊    | 6.60G/11.3G [01:37<01:01, 81.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  59%|█████▊    | 6.61G/11.3G [01:37<00:58, 86.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  59%|█████▊    | 6.62G/11.3G [01:37<00:55, 90.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  59%|█████▉    | 6.63G/11.3G [01:38<00:57, 87.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  59%|█████▉    | 6.63G/11.3G [01:38<00:58, 85.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  59%|█████▉    | 6.64G/11.3G [01:38<01:31, 54.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  59%|█████▉    | 6.65G/11.3G [01:38<01:18, 62.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  59%|█████▉    | 6.66G/11.3G [01:38<01:10, 70.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  59%|█████▉    | 6.67G/11.3G [01:38<01:04, 76.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  59%|█████▉    | 6.68G/11.3G [01:38<01:06, 73.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  59%|█████▉    | 6.69G/11.3G [01:39<01:01, 79.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  59%|█████▉    | 6.70G/11.3G [01:39<00:58, 84.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  59%|█████▉    | 6.70G/11.3G [01:39<01:01, 80.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  60%|█████▉    | 6.71G/11.3G [01:39<01:01, 80.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  60%|█████▉    | 6.72G/11.3G [01:39<01:00, 80.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  60%|█████▉    | 6.73G/11.3G [01:39<01:01, 79.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  60%|█████▉    | 6.73G/11.3G [01:39<01:00, 80.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  60%|█████▉    | 6.74G/11.3G [01:39<01:00, 80.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  60%|█████▉    | 6.75G/11.3G [01:39<01:00, 80.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  60%|█████▉    | 6.76G/11.3G [01:40<00:59, 81.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  60%|██████    | 6.77G/11.3G [01:40<00:58, 82.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  60%|██████    | 6.77G/11.3G [01:40<00:58, 82.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  60%|██████    | 6.78G/11.3G [01:40<00:58, 82.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  60%|██████    | 6.79G/11.3G [01:40<00:59, 80.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  60%|██████    | 6.80G/11.3G [01:40<01:01, 77.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  60%|██████    | 6.80G/11.3G [01:40<00:59, 80.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  60%|██████    | 6.81G/11.3G [01:40<00:58, 81.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  60%|██████    | 6.82G/11.3G [01:40<01:02, 76.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  61%|██████    | 6.83G/11.3G [01:40<01:03, 75.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  61%|██████    | 6.84G/11.3G [01:41<01:00, 78.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  61%|██████    | 6.84G/11.3G [01:41<01:04, 73.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  61%|██████    | 6.85G/11.3G [01:41<01:06, 71.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  61%|██████    | 6.86G/11.3G [01:41<01:05, 72.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  61%|██████    | 6.86G/11.3G [01:41<01:03, 74.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  61%|██████    | 6.87G/11.3G [01:41<01:02, 75.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  61%|██████    | 6.88G/11.3G [01:41<01:03, 74.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  61%|██████    | 6.89G/11.3G [01:41<01:02, 75.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  61%|██████    | 6.89G/11.3G [01:41<01:03, 74.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  61%|██████    | 6.90G/11.3G [01:42<01:01, 76.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  61%|██████▏   | 6.91G/11.3G [01:42<01:01, 76.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  61%|██████▏   | 6.92G/11.3G [01:42<01:00, 77.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  61%|██████▏   | 6.92G/11.3G [01:42<01:00, 77.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  61%|██████▏   | 6.93G/11.3G [01:42<00:58, 79.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  62%|██████▏   | 6.94G/11.3G [01:42<00:57, 81.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  62%|██████▏   | 6.95G/11.3G [01:42<00:58, 80.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  62%|██████▏   | 6.95G/11.3G [01:42<00:57, 80.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  62%|██████▏   | 6.96G/11.3G [01:42<00:55, 83.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  62%|██████▏   | 6.97G/11.3G [01:42<00:54, 84.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  62%|██████▏   | 6.98G/11.3G [01:43<00:55, 82.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  62%|██████▏   | 6.99G/11.3G [01:43<00:56, 80.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  62%|██████▏   | 6.99G/11.3G [01:43<00:55, 82.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  62%|██████▏   | 7.00G/11.3G [01:43<00:54, 84.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  62%|██████▏   | 7.01G/11.3G [01:43<00:55, 82.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  62%|██████▏   | 7.02G/11.3G [01:43<00:54, 83.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  62%|██████▏   | 7.03G/11.3G [01:43<00:57, 79.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  62%|██████▏   | 7.03G/11.3G [01:43<00:54, 83.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  62%|██████▏   | 7.04G/11.3G [01:43<00:56, 80.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  63%|██████▎   | 7.05G/11.3G [01:43<00:55, 81.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  63%|██████▎   | 7.06G/11.3G [01:44<01:03, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  63%|██████▎   | 7.06G/11.3G [01:44<01:03, 71.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  63%|██████▎   | 7.07G/11.3G [01:44<00:59, 75.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  63%|██████▎   | 7.08G/11.3G [01:44<01:01, 73.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  63%|██████▎   | 7.09G/11.3G [01:44<00:58, 76.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  63%|██████▎   | 7.09G/11.3G [01:44<01:00, 74.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  63%|██████▎   | 7.10G/11.3G [01:44<01:02, 72.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  63%|██████▎   | 7.11G/11.3G [01:44<01:02, 71.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  63%|██████▎   | 7.12G/11.3G [01:44<01:00, 74.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  63%|██████▎   | 7.12G/11.3G [01:45<01:01, 72.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  63%|██████▎   | 7.13G/11.3G [01:45<01:03, 69.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  63%|██████▎   | 7.14G/11.3G [01:45<01:01, 72.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  63%|██████▎   | 7.14G/11.3G [01:45<01:02, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  63%|██████▎   | 7.15G/11.3G [01:45<01:01, 72.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  63%|██████▎   | 7.16G/11.3G [01:45<00:58, 75.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  64%|██████▎   | 7.16G/11.3G [01:45<00:57, 76.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  64%|██████▎   | 7.17G/11.3G [01:45<00:58, 75.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  64%|██████▎   | 7.18G/11.3G [01:45<00:56, 77.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  64%|██████▎   | 7.19G/11.3G [01:45<00:55, 78.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  64%|██████▍   | 7.19G/11.3G [01:46<00:57, 75.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  64%|██████▍   | 7.20G/11.3G [01:46<00:58, 74.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  64%|██████▍   | 7.21G/11.3G [01:46<00:58, 75.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  64%|██████▍   | 7.22G/11.3G [01:46<00:58, 74.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  64%|██████▍   | 7.22G/11.3G [01:46<00:57, 75.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  64%|██████▍   | 7.23G/11.3G [01:46<01:03, 68.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  64%|██████▍   | 7.24G/11.3G [01:46<01:00, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  64%|██████▍   | 7.24G/11.3G [01:46<00:59, 72.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  64%|██████▍   | 7.25G/11.3G [01:46<00:58, 73.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  64%|██████▍   | 7.26G/11.3G [01:47<01:01, 70.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  64%|██████▍   | 7.27G/11.3G [01:47<00:59, 71.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  65%|██████▍   | 7.27G/11.3G [01:47<00:57, 75.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  65%|██████▍   | 7.28G/11.3G [01:47<01:00, 70.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  65%|██████▍   | 7.29G/11.3G [01:47<01:00, 70.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  65%|██████▍   | 7.29G/11.3G [01:47<00:58, 73.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  65%|██████▍   | 7.30G/11.3G [01:47<00:56, 75.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  65%|██████▍   | 7.31G/11.3G [01:47<00:56, 75.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  65%|██████▍   | 7.32G/11.3G [01:47<00:56, 75.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  65%|██████▍   | 7.32G/11.3G [01:47<00:54, 77.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  65%|██████▌   | 7.33G/11.3G [01:48<00:53, 79.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  65%|██████▌   | 7.34G/11.3G [01:48<00:53, 78.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  65%|██████▌   | 7.35G/11.3G [01:48<00:52, 80.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  65%|██████▌   | 7.35G/11.3G [01:48<00:51, 81.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  65%|██████▌   | 7.36G/11.3G [01:48<00:50, 83.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  65%|██████▌   | 7.37G/11.3G [01:48<00:52, 79.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  65%|██████▌   | 7.38G/11.3G [01:48<00:53, 78.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  66%|██████▌   | 7.39G/11.3G [01:48<00:55, 75.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  66%|██████▌   | 7.39G/11.3G [01:48<00:53, 78.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  66%|██████▌   | 7.40G/11.3G [01:49<00:51, 80.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  66%|██████▌   | 7.41G/11.3G [01:49<00:58, 70.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  66%|██████▌   | 7.42G/11.3G [01:49<00:55, 74.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  66%|██████▌   | 7.42G/11.3G [01:49<00:55, 74.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  66%|██████▌   | 7.43G/11.3G [01:49<00:56, 73.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  66%|██████▌   | 7.44G/11.3G [01:49<00:52, 77.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  66%|██████▌   | 7.45G/11.3G [01:49<00:52, 79.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  66%|██████▌   | 7.46G/11.3G [01:49<00:50, 81.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  66%|██████▌   | 7.46G/11.3G [01:49<00:54, 74.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  66%|██████▋   | 7.47G/11.3G [01:50<00:52, 77.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  66%|██████▋   | 7.48G/11.3G [01:50<00:51, 78.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  66%|██████▋   | 7.49G/11.3G [01:50<00:52, 77.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  66%|██████▋   | 7.49G/11.3G [01:50<00:50, 80.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  67%|██████▋   | 7.50G/11.3G [01:50<00:52, 76.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  67%|██████▋   | 7.51G/11.3G [01:50<00:57, 69.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  67%|██████▋   | 7.52G/11.3G [01:50<00:56, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  67%|██████▋   | 7.52G/11.3G [01:50<00:55, 72.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  67%|██████▋   | 7.53G/11.3G [01:50<00:54, 73.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  67%|██████▋   | 7.54G/11.3G [01:50<00:52, 76.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  67%|██████▋   | 7.55G/11.3G [01:51<00:53, 74.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  67%|██████▋   | 7.55G/11.3G [01:51<01:03, 63.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  67%|██████▋   | 7.56G/11.3G [01:51<01:03, 63.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  67%|██████▋   | 7.56G/11.3G [01:51<01:05, 60.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  67%|██████▋   | 7.57G/11.3G [01:51<01:02, 63.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  67%|██████▋   | 7.58G/11.3G [01:51<00:57, 68.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  67%|██████▋   | 7.59G/11.3G [01:51<00:54, 72.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  67%|██████▋   | 7.59G/11.3G [01:51<00:51, 77.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  67%|██████▋   | 7.60G/11.3G [01:51<00:50, 78.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  68%|██████▊   | 7.61G/11.3G [01:52<00:48, 81.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  68%|██████▊   | 7.62G/11.3G [01:52<00:46, 85.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  68%|██████▊   | 7.63G/11.3G [01:52<00:46, 84.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  68%|██████▊   | 7.64G/11.3G [01:52<00:46, 83.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  68%|██████▊   | 7.64G/11.3G [01:52<00:46, 83.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  68%|██████▊   | 7.65G/11.3G [01:52<00:46, 83.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  68%|██████▊   | 7.66G/11.3G [01:52<00:46, 84.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  68%|██████▊   | 7.67G/11.3G [01:52<00:45, 84.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  68%|██████▊   | 7.68G/11.3G [01:52<00:45, 85.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  68%|██████▊   | 7.68G/11.3G [01:52<00:45, 84.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  68%|██████▊   | 7.69G/11.3G [01:53<00:46, 83.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  68%|██████▊   | 7.70G/11.3G [01:53<00:45, 84.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  68%|██████▊   | 7.71G/11.3G [01:53<00:46, 83.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  68%|██████▊   | 7.71G/11.3G [01:53<00:45, 83.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  68%|██████▊   | 7.72G/11.3G [01:53<01:12, 52.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  69%|██████▊   | 7.73G/11.3G [01:53<01:01, 61.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  69%|██████▊   | 7.74G/11.3G [01:53<00:54, 69.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  69%|██████▊   | 7.75G/11.3G [01:53<00:49, 76.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  69%|██████▉   | 7.76G/11.3G [01:54<00:46, 81.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  69%|██████▉   | 7.77G/11.3G [01:54<00:44, 84.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  69%|██████▉   | 7.78G/11.3G [01:54<00:43, 87.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  69%|██████▉   | 7.78G/11.3G [01:54<00:44, 84.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  69%|██████▉   | 7.79G/11.3G [01:54<00:44, 84.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  69%|██████▉   | 7.80G/11.3G [01:54<00:47, 78.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  69%|██████▉   | 7.81G/11.3G [01:54<00:46, 80.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  69%|██████▉   | 7.82G/11.3G [01:54<00:44, 83.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  69%|██████▉   | 7.83G/11.3G [01:54<00:43, 85.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  69%|██████▉   | 7.83G/11.3G [01:55<00:45, 81.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  70%|██████▉   | 7.84G/11.3G [01:55<00:43, 84.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  70%|██████▉   | 7.85G/11.3G [01:55<00:44, 83.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  70%|██████▉   | 7.86G/11.3G [01:55<00:44, 82.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  70%|██████▉   | 7.87G/11.3G [01:55<00:45, 81.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  70%|██████▉   | 7.87G/11.3G [01:55<00:45, 79.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  70%|██████▉   | 7.88G/11.3G [01:55<00:46, 78.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  70%|██████▉   | 7.89G/11.3G [01:55<00:48, 74.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  70%|███████   | 7.90G/11.3G [01:55<00:47, 76.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  70%|███████   | 7.90G/11.3G [01:56<00:46, 77.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  70%|███████   | 7.91G/11.3G [01:56<00:45, 80.0MB/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  70%|███████   | 7.92G/11.3G [01:56<00:44, 80.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  70%|███████   | 7.93G/11.3G [01:56<00:43, 82.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  70%|███████   | 7.93G/11.3G [01:56<00:43, 82.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  70%|███████   | 7.94G/11.3G [01:56<00:43, 82.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  71%|███████   | 7.95G/11.3G [01:56<00:44, 79.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  71%|███████   | 7.96G/11.3G [01:56<00:48, 73.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  71%|███████   | 7.97G/11.3G [01:56<00:46, 76.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  71%|███████   | 7.97G/11.3G [01:56<00:43, 81.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  71%|███████   | 7.98G/11.3G [01:57<00:45, 78.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  71%|███████   | 7.99G/11.3G [01:57<00:44, 79.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  71%|███████   | 8.00G/11.3G [01:57<00:46, 76.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  71%|███████   | 8.00G/11.3G [01:57<00:46, 74.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  71%|███████   | 8.01G/11.3G [01:57<00:45, 77.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  71%|███████   | 8.02G/11.3G [01:57<00:45, 77.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  71%|███████   | 8.03G/11.3G [01:57<00:44, 78.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  71%|███████▏  | 8.03G/11.3G [01:57<00:42, 81.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  71%|███████▏  | 8.04G/11.3G [01:57<00:42, 81.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  71%|███████▏  | 8.05G/11.3G [01:57<00:42, 81.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  71%|███████▏  | 8.06G/11.3G [01:58<00:41, 82.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  72%|███████▏  | 8.07G/11.3G [01:58<00:49, 69.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  72%|███████▏  | 8.07G/11.3G [01:58<00:47, 72.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  72%|███████▏  | 8.08G/11.3G [01:58<00:45, 75.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  72%|███████▏  | 8.09G/11.3G [01:58<00:46, 74.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  72%|███████▏  | 8.10G/11.3G [01:58<00:43, 78.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  72%|███████▏  | 8.11G/11.3G [01:58<00:41, 82.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  72%|███████▏  | 8.11G/11.3G [01:58<00:40, 83.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  72%|███████▏  | 8.12G/11.3G [01:58<00:43, 77.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  72%|███████▏  | 8.13G/11.3G [01:59<00:42, 78.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  72%|███████▏  | 8.14G/11.3G [01:59<00:42, 80.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  72%|███████▏  | 8.14G/11.3G [01:59<00:42, 79.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  72%|███████▏  | 8.15G/11.3G [01:59<00:41, 81.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  72%|███████▏  | 8.16G/11.3G [01:59<00:42, 77.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  72%|███████▏  | 8.17G/11.3G [01:59<00:43, 76.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  73%|███████▎  | 8.18G/11.3G [01:59<00:42, 79.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  73%|███████▎  | 8.18G/11.3G [01:59<00:56, 59.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  73%|███████▎  | 8.19G/11.3G [02:00<00:52, 62.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  73%|███████▎  | 8.20G/11.3G [02:00<00:48, 68.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  73%|███████▎  | 8.21G/11.3G [02:00<00:44, 74.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  73%|███████▎  | 8.21G/11.3G [02:00<00:43, 76.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  73%|███████▎  | 8.22G/11.3G [02:00<00:41, 78.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  73%|███████▎  | 8.23G/11.3G [02:00<00:41, 78.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  73%|███████▎  | 8.24G/11.3G [02:00<00:39, 82.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  73%|███████▎  | 8.25G/11.3G [02:00<00:40, 81.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  73%|███████▎  | 8.25G/11.3G [02:00<00:38, 85.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  73%|███████▎  | 8.26G/11.3G [02:00<00:36, 87.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  73%|███████▎  | 8.27G/11.3G [02:01<00:36, 87.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  73%|███████▎  | 8.28G/11.3G [02:01<00:37, 85.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  74%|███████▎  | 8.29G/11.3G [02:01<00:39, 81.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  74%|███████▎  | 8.29G/11.3G [02:01<00:44, 72.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  74%|███████▎  | 8.30G/11.3G [02:01<00:45, 70.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  74%|███████▎  | 8.31G/11.3G [02:01<00:42, 74.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  74%|███████▍  | 8.32G/11.3G [02:01<00:40, 77.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  74%|███████▍  | 8.33G/11.3G [02:01<00:38, 81.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  74%|███████▍  | 8.33G/11.3G [02:01<00:38, 81.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  74%|███████▍  | 8.34G/11.3G [02:02<00:38, 82.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  74%|███████▍  | 8.35G/11.3G [02:02<00:37, 83.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  74%|███████▍  | 8.36G/11.3G [02:02<00:43, 72.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  74%|███████▍  | 8.37G/11.3G [02:02<00:42, 73.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  74%|███████▍  | 8.37G/11.3G [02:02<00:47, 65.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  74%|███████▍  | 8.38G/11.3G [02:02<00:48, 64.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  74%|███████▍  | 8.39G/11.3G [02:02<00:44, 69.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  74%|███████▍  | 8.39G/11.3G [02:02<00:41, 74.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  75%|███████▍  | 8.40G/11.3G [02:02<00:43, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  75%|███████▍  | 8.41G/11.3G [02:03<00:41, 74.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  75%|███████▍  | 8.42G/11.3G [02:03<00:39, 76.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  75%|███████▍  | 8.42G/11.3G [02:03<00:40, 75.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  75%|███████▍  | 8.43G/11.3G [02:03<00:39, 77.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  75%|███████▍  | 8.44G/11.3G [02:03<00:39, 76.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  75%|███████▍  | 8.45G/11.3G [02:03<00:38, 78.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  75%|███████▍  | 8.45G/11.3G [02:03<00:39, 76.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  75%|███████▌  | 8.46G/11.3G [02:03<00:39, 76.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  75%|███████▌  | 8.47G/11.3G [02:03<00:38, 78.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  75%|███████▌  | 8.48G/11.3G [02:03<00:36, 81.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  75%|███████▌  | 8.49G/11.3G [02:04<00:36, 83.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  75%|███████▌  | 8.49G/11.3G [02:04<00:38, 78.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  75%|███████▌  | 8.50G/11.3G [02:04<00:36, 82.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  75%|███████▌  | 8.51G/11.3G [02:04<00:36, 80.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  76%|███████▌  | 8.52G/11.3G [02:04<00:39, 74.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  76%|███████▌  | 8.52G/11.3G [02:04<00:38, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  76%|███████▌  | 8.53G/11.3G [02:04<00:38, 76.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  76%|███████▌  | 8.54G/11.3G [02:04<00:37, 78.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  76%|███████▌  | 8.55G/11.3G [02:04<00:36, 80.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  76%|███████▌  | 8.56G/11.3G [02:05<00:38, 76.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  76%|███████▌  | 8.56G/11.3G [02:05<00:36, 79.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  76%|███████▌  | 8.57G/11.3G [02:05<00:39, 73.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  76%|███████▌  | 8.58G/11.3G [02:05<00:37, 76.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  76%|███████▌  | 8.59G/11.3G [02:05<00:36, 79.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  76%|███████▌  | 8.59G/11.3G [02:05<00:35, 81.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  76%|███████▋  | 8.60G/11.3G [02:05<00:34, 84.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  76%|███████▋  | 8.61G/11.3G [02:05<00:41, 68.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  76%|███████▋  | 8.62G/11.3G [02:05<00:39, 72.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:  77%|███████▋  | 8.63G/11.3G [02:06<00:37, 76.6MB/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "smp_estimator.fit(\n",
    "    inputs=data_channels,\n",
    "    experiment_config={\n",
    "        \"ExperimentName\": experiment.experiment_name,\n",
    "        \"TrialName\": trial.trial_name,\n",
    "        \"TrialComponentDisplayName\": \"Training\",\n",
    "    },\n",
    "    logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_location = smp_estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the Training Logs\n",
    "\n",
    "You can access the training logs from [Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html). Make sure to look at the logs of algo-1 as that is the master node whose output stream will have the training job logs.\n",
    "\n",
    "You can use CloudWatch to track SageMaker GPU and memory utilization during training and inference. To view the metrics and logs that SageMaker writes to CloudWatch, see *Processing Job, Training Job, Batch Transform Job, and Endpoint Instance Metrics* in [Monitor Amazon SageMaker with Amazon CloudWatch](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html).\n",
    "\n",
    "If you are a new user of CloudWatch, see [Getting Started with Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/GettingStarted.html). \n",
    "\n",
    "For additional information on monitoring and analyzing Amazon SageMaker training jobs, see [Monitor and Analyze Training Jobs Using Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html).\n",
    "\n",
    "# Deploying Trained Model for Inference\n",
    "\n",
    "In most cases the trained model can be deployed on a single device for inference, since inference has smaller memory requirements. You can use the SMP API to create a single, unified model after training. For TensorFlow, a SavedModel can be created using `smp.DistributedModel.save_model` API, and for PyTorch, `smp.save()` can be used.\n",
    "\n",
    "After you build and train your models, you can deploy them to get predictions in one of two ways:\n",
    "\n",
    "* To set up a persistent endpoint to get predictions from your models, use SageMaker hosting services. For an overview on deploying a single model or multiple models with SageMaker hosting services, see [Deploy a Model on SageMaker Hosting Services](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html#how-it-works-hosting).\n",
    "* To get predictions for an entire dataset, use SageMaker batch transform. For an overview on deploying a model with SageMaker batch transform, see [Get Inferences for an Entire Dataset with Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html).\n",
    "\n",
    "To learn more about deploying models for inference using SageMaker, see [Deploy Models for Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model using `model_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker \n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "model_data = model_location\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data = model_data,  # path to your trained sagemaker model\n",
    "    role = role, # iam role with permissions to create an Endpoint\n",
    "    transformers_version = \"4.17\", # transformers version used\n",
    "    pytorch_version = \"1.10\", # pytorch version used\n",
    "    py_version = \"py38\", # python version of the DLC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count = 1,\n",
    "   instance_type = \"ml.m5.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example request, you always need to define \"inputs\"\n",
    "data = {\n",
    "   \"inputs\": \"The new Hugging Face SageMaker DLC makes it super easy to deploy models in production. It is great!\"\n",
    "}\n",
    "\n",
    "# request\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict({\n",
    "    'inputs': \"Can you please let us know more details about your \"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor.predict({\n",
    "    'inputs': \"Can you please let us know more \",\n",
    "  \"parameters\" : {\n",
    "    \"min_length\": 220,\n",
    "    \"temperature\": 0.6,\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "end_sequence = \".\"\n",
    "temparature = 40\n",
    "max_generated_token_length = 100\n",
    "input = \"Can you please let us know more details about your \"\n",
    "\n",
    "predictor.predict({\n",
    "    'inputs': input,\n",
    "    \"parameters\" : {\n",
    "        \"min_length\": int(len(input) + max_generated_token_length),\n",
    "        \"temperature\":temparature,\n",
    "        \"eos_token_id\": tokenizer.convert_tokens_to_ids(end_sequence)\n",
    "      }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
