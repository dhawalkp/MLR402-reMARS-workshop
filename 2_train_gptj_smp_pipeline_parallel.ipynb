{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ec0cdd4",
   "metadata": {},
   "source": [
    "# Train EleutherAI GPT-J with PyTorch 1.8.1 and Pipeline Parallelism Using the SageMaker Model Parallelism Library\n",
    "\n",
    "**This training job completes successfully on 1x p4dn.24xlarge instance or a cluster 4x p4.16xlarge instances.**\n",
    "\n",
    "*Please run this notebook with Data Science-> Python 3 Kernel on SageMaker Studio Notebook or a conda_pytorch_p38 Kernel on SageMaker Notebook instances*\n",
    "\n",
    "This notebook walks you through how to train the [EleutherAI's](https://www.eleuther.ai/) [GPT-J](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/) model with SageMaker's model parallelism.\n",
    "EleutherAI released GPT-J 6B, an open-source alternative to [OpenAIs GPT-3](https://openai.com/blog/gpt-3-apps/). [GPT-J 6B](https://huggingface.co/EleutherAI/gpt-j-6B) is the 6 billion parameter successor to EleutherAIs GPT-NEO family, a family of transformer-based language models based on the GPT architecture for text generation.\n",
    "\n",
    "EleutherAI's primary goal is to train a model that is equivalent in size to GPTâ -â 3 and make it available to the public under an open license.\n",
    "Over the last few months, GPT-J gained a lot of interest from Researchers, Data Scientists, and even Software Developers, but it remained very challenging to fine tune GPT-J.\n",
    "\n",
    "The weights of the 6 billion parameter model represent a ~24GB memory footprint. To load it in float32, one would need at least 2x model size CPU RAM: 1x for initial weights and another 1x to load the checkpoint. Apart from the model parameters, there are the gradients, optimizer states, and activations taking memory, so the actual memory usage might be significantly higher than 48GB. Just as an example, with Adam optimizer and FP32 training, the use from parameters, gradients and optimizer states might be 96GB+, and activation memory footprint would be even more than this, so the total memory usage might be easily larger than 200 GB.\n",
    "\n",
    "In this notebook, you will learn how to easily fine tune GPT-J using Amazon SageMaker and Hugging Face on NVIDIA GPU instances.\n",
    "\n",
    "This notebook depends on the following files and folders:\n",
    "\n",
    "1. `train_gptj_smp_script.py`: This is an entrypoint script that is passed to the PyTorch estimator in the notebook instructions. This script is responsible for end to end training of the GPT-J model with SMP. The script has additional comments at places where the SMP API is used.\n",
    "2. `fp16`: This folder is used for 16-bit float training, which contains a fp16 optimizer and various fp16 utilities.\n",
    "3. `learning_rates.py`: This contains the functions for learning rate schedule.\n",
    "4. `requirements.txt`: This will install the dependencies, like the right version of huggingface transformers.\n",
    "5. `preprocess.py`: This will download and preprocess the sst2/glue dataset.\n",
    "6. `args.py`: collection of difference arguments like training, data, SageMaker Model Parallel related args.\n",
    "7. `smp_trainer.py`.py: Defines the SageMaker Model Parallel Trainer class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4937ba6",
   "metadata": {},
   "source": [
    "## SageMaker Distributed Training \n",
    "\n",
    "SageMaker provides distributed training libraries for data parallelism and model parallelism. The libraries are optimized for the SageMaker training environment, help adapt your distributed training jobs to SageMaker, and improve training speed and throughput.\n",
    "\n",
    "### Approaches\n",
    "\n",
    "![SageMaker Distributed Training Approaches](img/TypesOfDistributedTraining.png)\n",
    "\n",
    "\n",
    "### SageMaker Model Parallel\n",
    "\n",
    "Model parallelism is the process of splitting a model up between multiple devices or nodes (such as GPU-equipped instances) and creating an efficient pipeline to train the model across these devices to maximize GPU utilization.\n",
    "\n",
    "Increasing deep learning model size (layers and parameters) can result in better accuracy. However, there is a limit to the maximum model size you can fit in a single GPU. When training deep learning models, GPU memory limitations can be a bottleneck in the following ways:\n",
    "\n",
    "1. They can limit the size of the model you train. Given that larger models tend to achieve higher accuracy, this directly translates to trained model accuracy.\n",
    "\n",
    "2. They can limit the batch size you train with, leading to lower GPU utilization and slower training.\n",
    "\n",
    "To overcome the limitations associated with training a model on a single GPU, you can use model parallelism to distribute and train your model on multiple computing devices.\n",
    "\n",
    "### Core features of SageMaker Model Parallel \n",
    "\n",
    "1. [Automated Model Splitting](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html): When you use SageMaker's model parallel library, you can take advantage of automated model splitting, also referred to as automated model partitioning. The library uses a partitioning algorithm that balances memory, minimizes communication between devices, and optimizes performance. You can configure the automated partitioning algorithm to optimize for speed or memory.\n",
    "\n",
    "2. [Pipeline Execution Schedule](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html): A core feature of SageMaker's distributed model parallel library is pipelined execution, which determines the order in which computations are made and data is processed across devices during model training. Pipelining is a technique to achieve true parallelization in model parallelism, by having the GPUs compute simultaneously on different data samples, and to overcome the performance loss due to sequential computation.\n",
    "\n",
    "Pipelining is based on splitting a mini-batch into microbatches, which are fed into the training pipeline one-by-one and follow an execution schedule defined by the library runtime. A microbatch is a smaller subset of a given training mini-batch. The pipeline schedule determines which microbatch is executed by which device for every time slot.\n",
    "\n",
    "In addition to its [core features](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html), the SageMaker distributed model parallel library offers [memory-saving features](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch.html) for training deep learning models with PyTorch: [tensor parallelism](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-tensor-parallelism.html), [optimizer state sharding](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-optimizer-state-sharding.html), [activation checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-checkpointing.html), and [activation offloading](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-offloading.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b774ec",
   "metadata": {},
   "source": [
    "### SageMaker Model Parallel configuration\n",
    "\n",
    "Please refer to all the [configuration parameters](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html) related to SageMaker Distributed Training.\n",
    "\n",
    "As we are going to use PyTorch and Hugging Face for training GPT-J, it is important to understand all the SageMaker Distributed configuration parameters specific to PyTorch [here](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html#pytorch-specific-parameters).\n",
    "\n",
    "#### Important\n",
    "\n",
    "`process_per_host` must not be greater than the number of GPUs per instance and typically will be equal to the number of GPUs per instance.\n",
    "\n",
    "For example, if you use one instance with 4-way pipeline parallelism and 2-way data parallelism, then processes_per_host should be 2 x 4 = 8. Therefore, you must choose an instance that has at least 8 GPUs, such as an ml.p3.16xlarge.\n",
    "\n",
    "The following image illustrates how 4-way data parallelism and 2-way pipeline parallelism is distributed across 8 GPUs: the models is partitioned across 2 GPUs, and each partition is added to 4 GPUs.\n",
    "\n",
    "It is also important to understand how the [ranking mechanism](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-ranking-mechanism.html) of model parallelism works with tensor parallelism. This is extended from the Ranking Basics for Core Features of the SageMaker Model Parallel Library.\n",
    "\n",
    "![SageMaker Distributed Training Approaches](img/SMP-Pipeline-Parallel-DDP.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012956da",
   "metadata": {},
   "source": [
    "#### Additional Resources\n",
    "If you are a new user of Amazon SageMaker, you may find the following helpful to learn more about SMP and using SageMaker with PyTorch.\n",
    "\n",
    "1. To learn more about the SageMaker model parallelism library, see [Model Parallel Distributed Training with SageMaker Distributed](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html).\n",
    "\n",
    "2. To learn more about using the SageMaker Python SDK with PyTorch, see Using [PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html).\n",
    "\n",
    "3. To learn more about launching a training job in Amazon SageMaker with your own training image, see [Use Your Own Training Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a8b6e4",
   "metadata": {},
   "source": [
    "### Using this notebook\n",
    "\n",
    "*Note:* You have two options on how to run this notebook.  \n",
    "    1. You can use the _Amazon SageMaker local mode_ to train on your notebook instance  \n",
    "    2. You can the _managed Amazon SageMaker training_ environment to train your model in a separate cluster of EC2 instance(s)\n",
    "\n",
    "The below cells check the notebook instance you are using and set the `local_training` flag to `True` if you are on a multi-GPU SageMaker notebook instance and can therefore train the model locally. If you have a CPU based SageMaker notebook instance we set the `local_training` flag to `False` and you will use SageMaker managed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e18c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def get_notebook_name():\n",
    "    import json\n",
    "    log_path = '/opt/ml/metadata/resource-metadata.json'\n",
    "    with open(log_path, 'r') as logs:\n",
    "        _logs = json.load(logs)\n",
    "    return _logs['ResourceName']\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "response = client.describe_notebook_instance(\n",
    "    NotebookInstanceName=get_notebook_name())\n",
    "    # set to the number of GPUs on that instance\n",
    "instance_type= response['InstanceType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f78dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if instance_type in ['ml.p4d.24xlarge']:\n",
    "    local_training = True\n",
    "    print(\"You are running this notebook on a multi GPU SageMaker notebook instance. For the purpose of this workshop you will use SageMaker local training.\")\n",
    "else:\n",
    "    local_training = False\n",
    "    print(\"You are running this notebook on a CPU based SageMaker notebook instance. For the purpose of this workshop you will use SageMaker remote/managed training.\")\n",
    "%store local_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258fae94",
   "metadata": {},
   "source": [
    "## Install and Upgrade Libraries\n",
    "\n",
    "The SageMaker model parallelism library's tensor parallelism feature requires the SageMaker Python SDK and the SageMaker Experiments library. Run the following cell to install or upgrade the libraries.\n",
    "\n",
    "**Note:** To finish applying the changes, you must restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd333b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p27\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/chainer_p\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/chainer_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/chainer_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/mxnet_latest_p37/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/mxnet_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/mxnet_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/python2/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/python3/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/pytorch_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/pytorch_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/tensorflow2_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/tensorflow_p27/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/tensorflow_p36/\n",
    "!rm -rf /home/ec2-user/anaconda3/envs/R/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9880f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once, restart kernel, then comment out this cell\n",
    "# update sagemaker to the latest 2.x version\n",
    "! pip install -qU pip\n",
    "! pip install -qU \"sagemaker>=2,<3\"\n",
    "! pip install -q 'sagemaker[local]' --upgrade\n",
    "! pip install -qU sagemaker-experiments\n",
    "! pip install -q setuptools==59.5.0\n",
    "! pip install -qr requirements.txt\n",
    "! pip install -q transformers\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4007a3",
   "metadata": {},
   "source": [
    "<b>Important</b> After you run the above cell, comment it out for future runs.\n",
    "\n",
    "Now, restart the kernel, and come back here following all cells again.\n",
    "\n",
    "Import and check if the SageMaker Python SDK version is successfully set to the latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6826f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31d775a",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Initialization\n",
    "\n",
    "Throughout this example, you'll use a training script of GPT-J model and a dataset from Huggingface.\n",
    "\n",
    "Based on the `local_training` flag, you need to setup SageMaker differently. The below cells check this flag and initailize SageMaker appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed92c4f6",
   "metadata": {},
   "source": [
    "#### Considerations when setting up SageMaker local training \n",
    "\n",
    "If you want to train locally on this notebook the following changes are needed. Running the cells in this notebook will take care of these steps.\n",
    "\n",
    "1. Install `pip install 'sagemaker[local]' --upgrade` (installed above)\n",
    "2. create config.yml in the same folder as the notebook   \n",
    "    2.1 Make a directory `!mkdir ./.sagemaker`  \n",
    "    2.2 save this to config.yml  \n",
    "    \n",
    "    `%%writefile -a ./.sagemaker/config.yml`\n",
    "  \n",
    "    ` \n",
    "    local:  \n",
    "    container_root: /home/ec2-user/SageMaker/hf-gptj-remars-workshop/training/distributed_training/pytorch/model_parallel/gpt-j/tmp/  \n",
    "    `\n",
    "3. Create a local session and comment out `sagemaker_session`\n",
    "\n",
    "    `\n",
    "    from sagemaker.local.local_session import LocalSession\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'container_root': '/home/ec2-user/SageMaker/hf-gptj-remars-workshop/training/distributed_training/pytorch/model_parallel/gpt-j/tmp/'}}\n",
    "    `\n",
    "  \n",
    "4. Comment out `checkpoint_s3_uri`, as it is not supported in local mode\n",
    "5. replace instance_type with `\"local_gpu\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43472c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve value of local_training\n",
    "%store -r local_training\n",
    "local_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_training:\n",
    "    # clear all images\n",
    "    !docker system prune -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_training:\n",
    "    !mkdir ./.sagemaker\n",
    "    !mkdir ./tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff98819",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./.sagemaker/config.yml\n",
    "\n",
    "local:\n",
    "#     local_code: true\n",
    "    container_root: /home/ec2-user/SageMaker/hf-gptj-remars-workshop/training/distributed_training/pytorch/model_parallel/gpt-j/tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dbfa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if local_training:\n",
    "    from sagemaker.local.local_session import LocalSession\n",
    "    sagemaker_session = LocalSession()\n",
    "    tmp_dir = os.getcwd() + \"/tmp/\"\n",
    "    sagemaker_session.config = {'local': {'container_root': tmp_dir,'local_code': True}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d577bc9",
   "metadata": {},
   "source": [
    "Run the following cell to import SageMaker modules and retrieve information of your current SageMaker work environment: your AWS account ID, the AWS Region you are using to run the notebook, and the ARN of your Amazon SageMaker execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356db19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "import boto3\n",
    "\n",
    "def get_notebook_name():\n",
    "    import json\n",
    "    log_path = '/opt/ml/metadata/resource-metadata.json'\n",
    "    with open(log_path, 'r') as logs:\n",
    "        _logs = json.load(logs)\n",
    "    return _logs['ResourceName']\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account: {account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region: {region}\")\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\")\n",
    "if not local_training:\n",
    "    sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "# get default bucket\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print()\n",
    "print(\"Default bucket for this session: \", default_bucket)\n",
    "\n",
    "response = sm_boto_client.describe_notebook_instance(\n",
    "    NotebookInstanceName=get_notebook_name())\n",
    "# set to the number of GPUs on that instance\n",
    "instance_type= response['InstanceType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd29a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_bucket = f\"s3://sagemaker-{region}-{account}/smp-model-parallel-outputdir/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a759d79d",
   "metadata": {},
   "source": [
    "## Training Dataset\n",
    "The training script fine-tunes GPT-J on the `sst2` dataset. The DataLoader and Sampler is defined in `smp_trainer.py`\n",
    "\n",
    "## Setup Hyperparameters\n",
    "The following `hyperparameters` dictionary is to pass arguments to the training script and set the model parallel configuration when creating the training job.\n",
    "\n",
    "You can also add custom mpi flags. By default, we have `--mca btl_vader_single_copy_mechanism none` to remove unnecessary logs.\n",
    "\n",
    "Next we add a base metric definitions to enable the metric upload in SageMaker. You can add any further metric definitions.\n",
    "\n",
    "We can train on multi-node clusters too, but the default below is on a single node so there's no capacity concerns. Note the following relationship when we are using pipeline parallelism. Note the flag below for ddp in the hyperparameters, which integrates with Pytorch's Distributed Data Parallel for data parallelism.\n",
    "\n",
    "`(pipeline parallelism degree) x (data parallelism degree) = total number of GPUs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58298be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = \"gpt-j-6B\"\n",
    "model_name_or_path = \"EleutherAI/gpt-j-6B\"\n",
    "\n",
    "if model_config == \"gpt-j-6B\":\n",
    "    model_params = {\n",
    "        \"tensor_parallel_degree\": 1,\n",
    "        \"pipeline_parallel_degree\": 8,\n",
    "        \"prescaled_batch\": 0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3401f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"dataset_name\": \"glue\",\n",
    "    \"dataset_config_name\": \"sst2\",\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": False,\n",
    "    \"load_from_s3\": False,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"output_dir\": \"./temp\",\n",
    "    \"model_name_or_path\": model_name_or_path,\n",
    "    \"max_steps\": 100,\n",
    "    \"seed\": 12345,\n",
    "    \"lr\": 2.0e-4,\n",
    "    \"lr_decay_iters\": 125000,\n",
    "    \"min_lr\": 0.00001,\n",
    "    \"warmup\": 0.01,\n",
    "    \"shard_optimizer_state\": 1,\n",
    "    \"activation_checkpointing\": 1,\n",
    "    \"activation_strategy\": \"each\",\n",
    "    \"optimize\": \"memory\",\n",
    "    \"ddp\": True,\n",
    "    \"cache_dir\": \"/tmp\",\n",
    "    \"save_final_full_model\": 1,\n",
    "}\n",
    "\n",
    "for k, v in model_params.items():\n",
    "    hyperparameters[k] = v\n",
    "    \n",
    "mpioptions = \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR \"\n",
    "mpioptions += \"-x SMP_NCCL_THROTTLE_LIMIT=1 \"\n",
    "mpioptions += \"-x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\"\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"base_metric\", \"Regex\": \"<><><><><><>\"}\n",
    "]  # Add your custom metric definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6918657",
   "metadata": {},
   "source": [
    "## Set Up SageMaker Studio Experiment\n",
    "Create or load [SageMaker Experiment](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) for the example training job. This will create an experiment trial object in SageMaker Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e921bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "# Specify your experiment name\n",
    "experiment_name = \"smp-gptj-pipeline-parallel\"\n",
    "# Specify your trial name\n",
    "trial_name = f\"{experiment_name}-trial1\"\n",
    "\n",
    "all_experiment_names = [exp.experiment_name for exp in Experiment.list()]\n",
    "# Load the experiment if it exists, otherwise create\n",
    "if experiment_name not in all_experiment_names:\n",
    "    experiment = Experiment.create(\n",
    "        experiment_name=experiment_name, sagemaker_boto_client=sm_boto_client\n",
    "    )\n",
    "else:\n",
    "    experiment = Experiment.load(\n",
    "        experiment_name=experiment_name, sagemaker_boto_client=sm_boto_client\n",
    "    )\n",
    "\n",
    "# Create the trial\n",
    "trial = Trial.create(\n",
    "    trial_name=\"smp-{}-{}\".format(trial_name, strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())),\n",
    "    experiment_name=experiment.experiment_name,\n",
    "    sagemaker_boto_client=sm_boto_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f9daa9",
   "metadata": {},
   "source": [
    "## Specify Essential Parameters for a SageMaker Training Job\n",
    "\n",
    "Next, you will use the [`SageMaker Estimator API`](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) to define a SageMaker Training Job, passing values through the following parameters for training job name, the number of EC2 instances, the instance type, and the size of the volume attached to the instances. \n",
    "\n",
    "* `instance_count`\n",
    "* `instance_type`\n",
    "* `volume_size`\n",
    "* `base_job_name`\n",
    "\n",
    "### Update the Type and Number of EC2 Instance to Use\n",
    "\n",
    "The instance type and the number of instances you specify to the `instance_type` and `instance_count` parameters, respectively, will determine the total number of GPUs (world size).\n",
    "\n",
    "$$ \\text{(world size) = (the number of GPUs on a single instance)}\\times\\text{(the number of instance)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f38d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define instance type for a remote training\n",
    "if not local_training:\n",
    "    instance_type = \"ml.p4d.24xlarge\"\n",
    "instance_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3322538",
   "metadata": {},
   "outputs": [],
   "source": [
    "if instance_type in ['ml.p3.16xlarge','p3dn.24xlarge','ml.g5.48xlarge', 'ml.p4d.24xlarge']:\n",
    "    processes_per_host = 8\n",
    "elif instance_type == 'ml.p2.16xlarge':\n",
    "    processes_per_host = 16\n",
    "else:\n",
    "    processes_per_host = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15320a",
   "metadata": {},
   "source": [
    "To look up the number of GPUs of different instance types, see [Amazon EC2 Instance Types](https://aws.amazon.com/ec2/instance-types/). Use the section **Accelerated Computing** to see general purpose GPU instances. Note that, for example, a given instance type `p4d.24xlarge` has a corresponding instance type `ml.p4d.24xlarge` in SageMaker.\n",
    "For SageMaker supported `ml` instances and cost information, see [Amazon SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a1e94",
   "metadata": {},
   "source": [
    "### Attach an EBS Volume to the Training Instance\n",
    "The volume size you specify in `volume_size` must be larger than your input data size. In this example, the volume size is set to 900GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224eb1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_size = 900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fef7a1",
   "metadata": {},
   "source": [
    "### Specify a Base Job Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bf7a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "pp_degree = hyperparameters[\"pipeline_parallel_degree\"]\n",
    "tp_degree = hyperparameters[\"tensor_parallel_degree\"]\n",
    "base_job_name = f'smp-{model_config}-{machine_str}-tp{tp_degree}-pp{pp_degree}-bs{hyperparameters[\"per_device_train_batch_size\"]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd3b89",
   "metadata": {},
   "source": [
    "### Create a SageMaker HuggingFace ðŸ¤— Estimator\n",
    "\n",
    "The following cell constructs a PyTorch estimator using the parameters defined above. To see how the SageMaker pipeline parallelism is enabled in the script, see the `train_gptj_smp_script.py` file and SageMaker Model Parallel documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b1fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_training:\n",
    "    instance_type = 'local_gpu'\n",
    "\n",
    "mpi = {\n",
    "    \"enabled\": True,\n",
    "    \"processes_per_host\": processes_per_host,\n",
    "    \"custom_mpi_options\": mpioptions,\n",
    "}\n",
    "\n",
    "smdistributed = {\n",
    "    \"modelparallel\": {\n",
    "        \"enabled\": True,\n",
    "        \"parameters\": {\n",
    "            \"ddp\": hyperparameters[\"ddp\"],\n",
    "            \"microbatches\": 2,\n",
    "            # partitions is a required param in the current SM SDK so it needs to be passed,\n",
    "            # these two map to the same config\n",
    "            \"partitions\": hyperparameters[\"pipeline_parallel_degree\"],\n",
    "            \"shard_optimizer_state\": hyperparameters[\"shard_optimizer_state\"] > 0,\n",
    "            \"prescaled_batch\": hyperparameters[\"prescaled_batch\"] > 0,\n",
    "            \"optimize\": hyperparameters[\"optimize\"],\n",
    "            \"auto_partition\": True,\n",
    "            \"default_partition\": 0,\n",
    "            \"offload_activations\": True,\n",
    "            \"active_microbatches\": 2,\n",
    "            \"optimize\": hyperparameters[\"optimize\"],\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "distribution = {\"mpi\": mpi, \"smdistributed\": smdistributed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec10431",
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_estimator = HuggingFace(\n",
    "    entry_point=\"train_gptj_smp_script.py\",\n",
    "    source_dir=os.getcwd(),\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=volume_size,\n",
    "    instance_count=instance_count,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    distribution=distribution,\n",
    "    pytorch_version=\"1.10.2\",\n",
    "    transformers_version=\"4.17.0\",\n",
    "    py_version=\"py38\",\n",
    "    output_path=s3_output_bucket,\n",
    "    hyperparameters=hyperparameters,\n",
    "    debugger_hook_config=False,\n",
    "    disable_profiler=True,\n",
    "    base_job_name=base_job_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b9305",
   "metadata": {},
   "source": [
    "Finally, run the estimator to launch the SageMaker training job of GPT-J model with pipeline parallelism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036a18a",
   "metadata": {},
   "source": [
    "If you receive a `ResourceLimitExceeded` error message when running the following cell, you can request an increase on the default quota by contacting [AWS support](https://console.aws.amazon.com/support). Open the [AWS Support Center](https://console.aws.amazon.com/support), and then choose Create case. Choose Service limit increase. For Limit Type choose SageMaker Training Jobs. Complete the rest of the form and submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de4c9a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smp_estimator.fit(\n",
    "    experiment_config={\n",
    "        \"ExperimentName\": experiment.experiment_name,\n",
    "        \"TrialName\": trial.trial_name,\n",
    "        \"TrialComponentDisplayName\": \"Training\",\n",
    "    },\n",
    "    logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e227d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_location = smp_estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ba1c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2cbe70",
   "metadata": {},
   "source": [
    "## Accessing the Training Logs\n",
    "\n",
    "You can access the training logs from [Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html). Make sure to look at the logs of **algo-1** because that is the main node whose output stream will have the training job logs.\n",
    "\n",
    "You can use CloudWatch to track SageMaker GPU and memory utilization during training and inference. To view the metrics and logs that SageMaker writes to CloudWatch, see [SageMaker Jobs and Endpoint Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html#cloudwatch-metrics-jobs) in the Amazon SageMaker Developer Guide.\n",
    "\n",
    "If you are a new user of CloudWatch, see [Getting Started with Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/GettingStarted.html). \n",
    "\n",
    "For additional information on monitoring and analyzing Amazon SageMaker training jobs, see [Monitor and Analyze Training Jobs Using Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html).\n",
    "\n",
    "## Deploying Trained Model for Inference\n",
    "\n",
    "In most cases, a trained model can be deployed on a single device for inference because inference only requires a small amount of memory. You can use the SMP API to create a single, unified model after training: See [smp.save()](https://sagemaker.readthedocs.io/en/stable/api/training/smp_versions/latest/smd_model_parallel_pytorch.html#apis-for-saving-and-loading) function for PyTorch.\n",
    "\n",
    "After you build and train your models, you can deploy them to get predictions in one of two ways:\n",
    "\n",
    "* To set up a persistent endpoint to get predictions from your models, use SageMaker hosting services. For an overview on deploying a single model or multiple models with SageMaker hosting services, see [Deploy a Model on SageMaker Hosting Services](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html#how-it-works-hosting).\n",
    "* To get predictions for an entire dataset, use SageMaker batch transform. For an overview on deploying a model with SageMaker Batch Transform, see [Get Inferences for an Entire Dataset with Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html).\n",
    "\n",
    "To learn more about deploying models for inference using SageMaker, see [Deploy Models for Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3767f2eb",
   "metadata": {},
   "source": [
    "### Deploy the model using `model_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3271169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker \n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "model_data = model_location\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data = model_data,  # path to your trained sagemaker model\n",
    "    role = role, # iam role with permissions to create an Endpoint\n",
    "    transformers_version = \"4.17\", # transformers version used\n",
    "    pytorch_version = \"1.10\", # pytorch version used\n",
    "    py_version = \"py38\", # python version of the DLC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1cb1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count = 1,\n",
    "   instance_type = \"ml.m5.4xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73489238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example request, you always need to define \"inputs\"\n",
    "data = {\n",
    "   \"inputs\": \"The new Hugging Face SageMaker DLC makes it super easy to deploy models in production. It is great!\"\n",
    "}\n",
    "\n",
    "# request\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd533b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict({\n",
    "    'inputs': \"Can you please let us know more details about your \"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e501e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict({\n",
    "    'inputs': \"Can you please let us know more details about your \"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e0ca0b",
   "metadata": {},
   "source": [
    "Parameterized request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b57d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor.predict({\n",
    "    'inputs': \"Can you please let us know more \",\n",
    "  \"parameters\" : {\n",
    "    \"min_length\": 220,\n",
    "    \"temperature\": 0.6,\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0da90",
   "metadata": {},
   "source": [
    "Custom end of sequence token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bba715",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "end_sequence = \".\"\n",
    "temparature = 40\n",
    "max_generated_token_length = 100\n",
    "input = \"Can you please let us know more details about your \"\n",
    "\n",
    "predictor.predict({\n",
    "    'inputs': input,\n",
    "    \"parameters\" : {\n",
    "        \"min_length\": int(len(input) + max_generated_token_length),\n",
    "        \"temperature\":temparature,\n",
    "        \"eos_token_id\": tokenizer.convert_tokens_to_ids(end_sequence)\n",
    "      }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
